{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "多目标推荐_full_doc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gipGcAr8K0Of"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgX4Io2dK2BV"
      },
      "source": [
        "https://digix-algo-challenge.obs.cn-east-2.myhuaweicloud.com/2021/AI/4d83d37a6bc6e83d0da3af019c18775c/2021_3_data.zip\n",
        "\n",
        "提供的数据包括用户维度、视频维度以及用户历史行为数据，以下按照这三个维度分别说明。\n",
        "\n",
        "\n",
        "### 用户特征\n",
        "\n",
        "| 字段名称 | 说明 | 是否为空 | 类型 | 取值样例 |\n",
        "| :------- | :--- | :------- | :--- | :------- |\n",
        "| user_id     | 用户ID   | 否   | string | 1    |\n",
        "| age         | 年龄段   | 是   | string | 0    |\n",
        "| gender      | 性别     | 是   | string | 1    |\n",
        "| country     | 国家     | 是   | string | 0    |\n",
        "| province    | 所在省份 | 是   | string | 0    |\n",
        "| city        | 所在城市 | 是   | string | 0    |\n",
        "| city_level  | 城市级别 | 是   | string | 0    |\n",
        "| device_name | 设备类型 | 是   | string | 0    |\n",
        "\n",
        "\n",
        "### 视频内容特征\n",
        "\n",
        "| 字段名称 | 说明 | 是否为空 | 类型 | 取值样例 |\n",
        "| :------- | :--- | :------- | :--- | :------- |\n",
        "| video_id            | 视频ID           | 否   | string | 16451                                                        |\n",
        "| video_name          | 视频名称         | 是   | string | 天下无贼                                                     |\n",
        "| video_tags          | 视频标签         | 是   | string | 扒手,反扒,t,tx,txm,txmz,txw,txwz,天下无贼,刘德华,刘若英,王宝强,冯小刚,张晞临,芒果TV,院线,剧情,李冰冰,葛优,动作,犯罪 |\n",
        "| video_description   | 视频描述         | 是   | string | 王薄（刘德华 饰）和王丽（刘若英 饰）本是一对最佳贼拍档，但因怀了王薄的孩子，王丽决定收手赎罪，两人产生分歧。在火车站遇到刚刚从城市里挣了一笔钱准备回老家用它盖房子娶媳妇的农村娃子傻根（王宝强 饰）后，王丽被他的单纯打动，决定暗中保护不使他的辛苦钱失窃，王薄却寻思找合适机会下手，但 最终因为“夫妻情深”归入了王丽的阵营。 不料傻根的钱早被以黎叔（葛优 饰）为头目的另一著名扒窃团伙盯上，于是一系列围绕傻根书包里的钞票、在王薄、王丽和黎叔团伙之间展开的强强斗争上演。 |\n",
        "| video_release_date  | 年代             | 是   | string | 2004/12/9                                                    |\n",
        "| video_director_list | 导演名称         | 是   | string | 冯小刚,林黎胜                                                |\n",
        "| video_actor_list    | 演员名称         | 是   | string | 刘德华,葛优,刘若英,王宝强                                    |\n",
        "| video_score         | 评分             | 是   | string | 8.5                                                          |\n",
        "| video_second_class  | 视频二级分类名称 | 是   | string | 喜剧,剧情,警匪,动作,犯罪                                     |\n",
        "| video_duration      | 视频时长，单位秒 | 是   | string | 7246                                                         |\n",
        "\n",
        "### 用户历史行为\n",
        "\n",
        "| 字段名称 | 说明 | 是否为空 | 类型 | 取值样例 |\n",
        "| :------- | :--- | :------- | :--- | :------- |\n",
        "| user_id          | 用户id             | 否   | string | 1         |\n",
        "| video_id         | 视频id             | 否   | string | 16451     |\n",
        "| is_watch         | 是否播放           | 是   | int    | 1         |\n",
        "| is_share         | 是否分享           | 是   | int    | 0         |\n",
        "| is_collect       | 是否收藏           | 是   | int    | 0         |\n",
        "| is_comment       | 是否评论           | 是   | int    | 0         |\n",
        "| watch_start_time | 观看时间           | 是   | string | 2020-11-3 |\n",
        "| watch_label      | 播放标签           | 是   | int    | 8         |\n",
        "| pt_d             | 时间，用于数据分区 | 是   | string | 20201103  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0PjL1QlLH4u"
      },
      "source": [
        "###目标\n",
        "\n",
        "预测每位用户对视频观看时长所在区间，且预测是否对视频进行分享。本次比赛使用 AUC（ROC曲线下面积）作为评估指标，AUC 越高，代表结果越优，排名越靠前。各指标的AUC采用加权求和。\n",
        "\n",
        "$$score = α*(0.1*AUC_{watch1}+ 0.2*AUC_{watch2}+0.3*AUC_{watch3}+0.4*AUC_{watch4}+0.5*AUC_{watch5}+0.6*AUC_{watch6}+0.7*AUC_{watch7} +0.8*AUC_{watch8} +0.9*AUC_{watch9} )+β*AUC_{share}$$\n",
        "\n",
        "α为0.7，β为0.3\n",
        "\n",
        "其中：视频观看时长、label、加权权重的对应关系如下。\n",
        "\n",
        "| 观看时长区间【左闭右开区间】 | watch_label | 加权权重 |\n",
        "| :--------------------------- | :---------- | :------- |\n",
        "| 0~10%总片长    | 0    | 0    |\n",
        "| 10~20%总片长   | 1    | 0.1  |\n",
        "| 20%~30%总片长  | 2    | 0.2  |\n",
        "| 30%~40%总片长  | 3    | 0.3  |\n",
        "| 40%~50%总片长  | 4    | 0.4  |\n",
        "| 50%~60%总片长  | 5    | 0.5  |\n",
        "| 60%~70%总片长  | 6    | 0.6  |\n",
        "| 70%~80%总片长  | 7    | 0.7  |\n",
        "| 80%~90%总片长  | 8    | 0.8  |\n",
        "| 90%~100%总片长 | 9    | 0.9  |\n",
        "\n",
        "#### 提交方式\n",
        "\n",
        "提交结果格式如下：user_id, video_id, watch_label, is_share\n",
        "\n",
        "其中user_id对应测试样本中的user_id，video_id对应测试样本的video_id，watch_label表示user_id观看video_id时长所在的时长区间，is_share表示用户是否对该视频进行了分享。\n",
        "\n",
        "提交文件格式参考如下示例：\n",
        "\n",
        "| user_id | video_id | watch_label | is_share |\n",
        "| :------ | :------- | :---------- | :------- |\n",
        "| 1    | 1    | 6    | 0    |\n",
        "| 1    | 1    | 9    | 1    |\n",
        "| ...  | ...  | ...  | ...  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ7_ytNJLmXN"
      },
      "source": [
        "### 数据分析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMFFKr5nKhOB"
      },
      "source": [
        "import pandas as pd\n",
        "import glob, gc\n",
        "\n",
        "%pylab inline\n",
        "import seaborn as sns\n",
        "\n",
        "INPUT_PATH = './3'\n",
        "\n",
        "def reduce_mem(df):\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    gc.collect()\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkdUEk-cLsm1"
      },
      "source": [
        "test_data = pd.read_csv(f'{INPUT_PATH}/testdata/test.csv', sep=',')\n",
        "user_features = pd.read_csv(f'{INPUT_PATH}/traindata/user_features_data/user_features_data.csv', sep='\\t')\n",
        "video_features = pd.read_csv(f'{INPUT_PATH}/traindata/video_features_data/video_features_data.csv', sep='\\t')\n",
        "history_behavior = pd.concat([pd.read_csv(x, sep='\\t')) for x in glob.glob(f'{INPUT_PATH}/traindata/history_behavior_data/*/*'])\n",
        "history_behavior = history_behavior.sort_values(by=['pt_d', 'user_id'])\n",
        "\n",
        "test_data = reduce_mem(test_data)\n",
        "user_features = reduce_mem(user_features)\n",
        "video_features = reduce_mem(video_features)\n",
        "history_behavior = reduce_mem(history_behavior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqYw6lT_QXE8"
      },
      "source": [
        "## for reload reduced data.    to an HDF5 file using HDFStore.\n",
        "# test_data.to_hdf('digix-data.hdf', 'test_data')\n",
        "# user_features.to_hdf('digix-data.hdf', 'user_features')\n",
        "# video_features.to_hdf('digix-data.hdf', 'video_features')\n",
        "# history_behavior.to_hdf('digix-data.hdf', 'history_behavior')\n",
        "\n",
        "## reload\n",
        "# test_data = pd.read_hdf('digix-data.hdf', 'test_data')\n",
        "# user_features = pd.read_hdf('digix-data.hdf', 'user_features')\n",
        "# video_features = pd.read_hdf('digix-data.hdf', 'video_features')\n",
        "# history_behavior = pd.read_hdf('digix-data.hdf', 'history_behavior')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFFFerE9QWq2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye6lG7-vL8W3"
      },
      "source": [
        "user_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN56fskLL93G"
      },
      "source": [
        "video_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZLDSMALL_eF"
      },
      "source": [
        "history_behavior.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alJOPzHjMBl9"
      },
      "source": [
        "user_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xaLi579MZal"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af26CgzbMF4M"
      },
      "source": [
        "for col in user_features.columns: \n",
        "    print(user_features.shape, user_features[col].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sdWlmR0MIL9"
      },
      "source": [
        "user_features['age'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNASVxzBMKnG"
      },
      "source": [
        "user_features['gender'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LncigMzJML90"
      },
      "source": [
        "user_features['country'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4x-3OXoMNPe"
      },
      "source": [
        "user_features['province'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTzu6X69MNJk"
      },
      "source": [
        "user_features['city'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zjZ-_FRMPvF"
      },
      "source": [
        "user_features['city_level'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqsUywpAMRFM"
      },
      "source": [
        "user_features['device_name'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17qFKh5MMSOX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krwvsPywMTTl"
      },
      "source": [
        "for col in history_behavior.columns: \n",
        "    print(history_behavior.shape, history_behavior[col].nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE8sPQncMXy9"
      },
      "source": [
        "len(set(test_data['user_id']) & set(history_behavior['user_id']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic6RcVoqMbYG"
      },
      "source": [
        "test_data['user_id'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEEXhWhMdJE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWwagfi7Ngw0"
      },
      "source": [
        "history_behavior[(history_behavior['user_id'] == 2) & (history_behavior['video_id'] == 25469)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0RYc3IMjXn"
      },
      "source": [
        "### 训练数据划分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZdAPv3_MfhH"
      },
      "source": [
        "history_behavior = history_behavior[history_behavior['user_id'].isin(test_data['user_id'].unique())]\n",
        "\n",
        "val_behavior = history_behavior[history_behavior['pt_d'] == 20210502]\n",
        "train_behavior = history_behavior[history_behavior['pt_d'] != 20210502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkcYcSuTMsq0"
      },
      "source": [
        "val_behavior = val_behavior.rename(columns={\"watch_label\": \"val_watch\", \"is_share\": \"val_share\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujGazCbMqEn"
      },
      "source": [
        "train_behavior = pd.merge(train_behavior,  val_behavior[['user_id', 'video_id', 'val_watch', 'val_share']], \n",
        "                                           on=['user_id', 'video_id'], how='left')\n",
        "\n",
        "train_behavior['val_watch'] = train_behavior['val_watch'].fillna(0)\n",
        "train_behavior['val_share'] = train_behavior['val_share'].fillna(0)\n",
        "\n",
        "# balance labeled and unlabeled\n",
        "train_behavior = pd.concat([\n",
        "    train_behavior[train_behavior['val_watch'] == 0].sample(50000),\n",
        "    train_behavior[train_behavior['val_watch'] != 0]\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqjit6fbM69F"
      },
      "source": [
        "train_behavior['val_watch'].value_counts()  # target for trian"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b-KR-mgM-RU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icK8ahCeM9tt"
      },
      "source": [
        "train_user_behavior_agg = train_behavior.groupby('user_id').agg({\n",
        "    'pt_d': ['count'],\n",
        "    'video_id': ['nunique'],\n",
        "    'is_watch': ['mean', 'max'],\n",
        "    'is_share': ['mean', 'max'],\n",
        "    'watch_label': ['nunique']\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfDto8gM-HU"
      },
      "source": [
        "# rename new feature\n",
        "train_user_behavior_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper()  for e in train_user_behavior_agg.columns.tolist()])\n",
        "train_user_behavior_agg = train_user_behavior_agg.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_np8MlNNU1-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVgBtN6ZNi7-"
      },
      "source": [
        "## Model One"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhPRyfNP_9r"
      },
      "source": [
        "这也太简单了"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYQ1W-3NOtgX"
      },
      "source": [
        "class MLPDataset(Dataset):\n",
        "    def __init__(self, history_behavior, train=True):\n",
        "        self.history_behavior = history_behavior\n",
        "        self.train = train\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        user_id = self.history_behavior.iloc[index]['user_id']\n",
        "        video_id = self.history_behavior.iloc[index]['video_id']\n",
        "        \n",
        "        user_province = user_features.loc[user_id]['province']\n",
        "        user_city = user_features.loc[user_id]['city']\n",
        "        user_device = user_features.loc[user_id]['device_name']\n",
        "        \n",
        "        feed_dict = {\n",
        "            'user_id': user_id,\n",
        "            'video_id': video_id,\n",
        "            'user_province': user_features.loc[user_id]['province'],\n",
        "            'user_city': user_features.loc[user_id]['city'],\n",
        "            'user_device': user_features.loc[user_id]['device_name'],\n",
        "            'user_age': user_features.loc[user_id]['age'],\n",
        "            'city_level': user_features.loc[user_id]['city_level'],\n",
        "        }\n",
        "        if self.train:\n",
        "            watch_label = self.history_behavior.iloc[index]['watch_label']\n",
        "            share_label = self.history_behavior.iloc[index]['is_share']\n",
        "            feed_dict['watch_label'] = watch_label\n",
        "            feed_dict['share_label'] = share_label\n",
        "        \n",
        "        return feed_dict\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.history_behavior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevQ_0u2Nl64"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"只是简单对 id 和 类别特征 进行一个embedding学习，MLP预测两个目标的label\"\n",
        "\n",
        "    def __init__(self, \n",
        "                        n_users=5910799, \n",
        "                        n_items=50356, \n",
        "                        n_provinces=34, \n",
        "                        n_citys=340, \n",
        "                        n_devices=1827,\n",
        "                        layers=[64, 32], dropout=False):\n",
        "        super().__init__()\n",
        "        self.user_embedding = torch.nn.Embedding(n_users, 32)\n",
        "        self.video_embedding = torch.nn.Embedding(n_items, 32)\n",
        "        self.user_province_embedding = torch.nn.Embedding(n_provinces, 5)\n",
        "        self.user_city_embedding = torch.nn.Embedding(n_citys, 5)\n",
        "        self.user_device_embedding = torch.nn.Embedding(n_devices, 5)\n",
        "\n",
        "        # list of weight matrices\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for _, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        self.output_layer1 = torch.nn.Linear(layers[-1], 10)\n",
        "        self.output_layer2 = torch.nn.Linear(layers[-1], 1)\n",
        "\n",
        "    def forward(self, feed_dict):\n",
        "        users = feed_dict['user_id']\n",
        "        items = feed_dict['video_id']\n",
        "        user_embedding = self.user_embedding(users)\n",
        "        video_embedding = self.video_embedding(items)\n",
        "        user_province = self.user_province_embedding(feed_dict['user_province'])\n",
        "        user_city = self.user_city_embedding(feed_dict['user_city'])\n",
        "        user_device = self.user_device_embedding(feed_dict['user_device'])\n",
        "        \n",
        "        # x = torch.cat([user_embedding, video_embedding], 1)\n",
        "        x = torch.cat([user_embedding, video_embedding, user_province, user_city, user_device], 1)\n",
        "        \n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            x = self.fc_layers[idx](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x)\n",
        "        logit1 = self.output_layer1(x)\n",
        "        logit2 = self.output_layer2(x)\n",
        "        return logit1, logit2\n",
        "\n",
        "    def predict(self, feed_dict):\n",
        "        # just deal with numpy inputs\n",
        "        for key in feed_dict:\n",
        "            if type(feed_dict[key]) != type(None):\n",
        "                feed_dict[key] = torch.from_numpy(feed_dict[key]).to(dtype=torch.long, device='cpu')\n",
        "\n",
        "        output_scores = self.forward(feed_dict)\n",
        "        return output_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txiTw5wnOqbg"
      },
      "source": [
        "model = MLP()\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9oeezb2Vk4L"
      },
      "source": [
        "## Model Two"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fazSrSoFVkZj"
      },
      "source": [
        "class DeepFM(nn.Module):\n",
        "    def __init__(self, cate_fea_nuniqs, nume_fea_size=0, emb_size=8, \n",
        "                        hid_dims=[256, 128], num_classes=1, dropout=[0.2, 0.2]): \n",
        "        \"\"\"\n",
        "        cate_fea_nuniqs: 类别特征 n unique name.\n",
        "        nume_fea_size: 数值特征个数\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cate_fea_size = len(cate_fea_nuniqs)\n",
        "        self.nume_fea_size = nume_fea_size\n",
        "        \n",
        "        \"\"\"FM部分\"\"\"\n",
        "        # 一阶\n",
        "        if self.nume_fea_size != 0:\n",
        "            self.fm_1st_order_dense = nn.Linear(self.nume_fea_size, 1)  # 数值特征的一阶表示\n",
        "        self.fm_1st_order_sparse_emb = nn.ModuleList([\n",
        "            nn.Embedding(voc_size, 1) for voc_size in cate_fea_nuniqs])  # 类别特征的一阶表示\n",
        "        \n",
        "        # 二阶\n",
        "        self.fm_2nd_order_sparse_emb = nn.ModuleList([\n",
        "            nn.Embedding(voc_size, emb_size) for voc_size in cate_fea_nuniqs])  # 类别特征的二阶表示\n",
        "        \n",
        "        \"\"\"DNN部分\"\"\"\n",
        "        self.all_dims = [self.cate_fea_size * emb_size] + hid_dims\n",
        "        self.dense_linear = nn.Linear(self.nume_fea_size, self.cate_fea_size * emb_size)  # 数值特征的维度变换到FM输出维度一致\n",
        "        self.relu = nn.ReLU()\n",
        "        # for DNN \n",
        "        for i in range(1, len(self.all_dims)):\n",
        "            setattr(self, 'linear_'+str(i), nn.Linear(self.all_dims[i-1], self.all_dims[i]))\n",
        "            setattr(self, 'batchNorm_' + str(i), nn.BatchNorm1d(self.all_dims[i]))\n",
        "            setattr(self, 'activation_' + str(i), nn.ReLU())\n",
        "            setattr(self, 'dropout_'+str(i), nn.Dropout(dropout[i-1]))\n",
        "        # for output \n",
        "        self.dnn_linear1 = nn.Linear(hid_dims[-1]+2, 10)  # 10 watch label\n",
        "        self.dnn_linear2 = nn.Linear(hid_dims[-1], 1)\n",
        "        \n",
        "    def forward(self, X_sparse, X_dense=None):\n",
        "        \"\"\"\n",
        "        X_sparse: 类别型特征输入  [bs, cate_fea_size]\n",
        "        X_dense: 数值型特征输入（可能没有）  [bs, dense_fea_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"FM 一阶部分\"\"\"\n",
        "        \n",
        "        print('self.fm_1st_order_sparse_emb', len(self.fm_1st_order_sparse_emb))\n",
        "        \n",
        "        fm_1st_sparse_res = [emb(X_sparse[:, i].unsqueeze(1)).view(-1, 1)  for i, emb in enumerate(self.fm_1st_order_sparse_emb)]\n",
        "        fm_1st_sparse_res = torch.cat(fm_1st_sparse_res, dim=1)  # [bs, cate_fea_size]\n",
        "        fm_1st_sparse_res = torch.sum(fm_1st_sparse_res, 1,  keepdim=True)  # [bs, 1]\n",
        "        \n",
        "        if X_dense is not None:\n",
        "            fm_1st_dense_res = self.fm_1st_order_dense(X_dense) \n",
        "            fm_1st_part = fm_1st_sparse_res + fm_1st_dense_res\n",
        "        else:\n",
        "            fm_1st_part = fm_1st_sparse_res   # [bs, 1]\n",
        "        \n",
        "        \"\"\"FM 二阶部分\"\"\"\n",
        "        fm_2nd_order_res = [emb(X_sparse[:, i].unsqueeze(1)) for i, emb in enumerate(self.fm_2nd_order_sparse_emb)]\n",
        "        fm_2nd_concat_1d = torch.cat(fm_2nd_order_res, dim=1)  # [bs, n, emb_size]  n为类别型特征个数(cate_fea_size)\n",
        "        \n",
        "        # 先求和再平方\n",
        "        sum_embed = torch.sum(fm_2nd_concat_1d, 1)  # [bs, emb_size]\n",
        "        square_sum_embed = sum_embed * sum_embed    # [bs, emb_size]\n",
        "        # 先平方再求和\n",
        "        square_embed = fm_2nd_concat_1d * fm_2nd_concat_1d  # [bs, n, emb_size]\n",
        "        sum_square_embed = torch.sum(square_embed, 1)  # [bs, emb_size]\n",
        "        # 相减除以2 \n",
        "        sub = square_sum_embed - sum_square_embed  \n",
        "        sub = sub * 0.5   # [bs, emb_size]\n",
        "        \n",
        "        fm_2nd_part = torch.sum(sub, 1, keepdim=True)   # [bs, 1]\n",
        "        \n",
        "        \"\"\"DNN部分\"\"\"\n",
        "        dnn_out = torch.flatten(fm_2nd_concat_1d, 1)   # [bs, n * emb_size]\n",
        "        \n",
        "        if X_dense is not None:\n",
        "            dense_out = self.relu(self.dense_linear(X_dense))   # [bs, n * emb_size]\n",
        "            dnn_out = dnn_out + dense_out   # [bs, n * emb_size]\n",
        "        \n",
        "        for i in range(1, len(self.all_dims)):\n",
        "            dnn_out = getattr(self, 'linear_' + str(i))(dnn_out)\n",
        "            dnn_out = getattr(self, 'batchNorm_' + str(i))(dnn_out)\n",
        "            dnn_out = getattr(self, 'activation_' + str(i))(dnn_out)\n",
        "            dnn_out = getattr(self, 'dropout_' + str(i))(dnn_out)\n",
        "        \n",
        "        # watch predict\n",
        "        out1 = self.dnn_linear1(torch.cat([dnn_out, fm_1st_part, fm_2nd_part],1))   # [bs, N]\n",
        "         \n",
        "         # share predict\n",
        "        dnn_out2 = self.dnn_linear2(dnn_out)   # [bs, 1]\n",
        "        out2 = fm_1st_part + fm_2nd_part + dnn_out2   # [bs, 1]\n",
        "        \n",
        "        return out1, out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWqsiDnVVoHr"
      },
      "source": [
        "model = DeepFM(cate_fea_nuniqs=[5910799, 50356,34, 340, 1927], nume_fea_size=2)\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dXPHlj_i4Oh"
      },
      "source": [
        "## Train\n",
        "\n",
        "NOTE: 将两个目标分开训练，而不是一个损失函数，效果更好。就是说，watch_loss_fn 和 share_loss_fn 分别训练。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK_2Y9mcOqYH"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = MLPDataset(train_behavior),\n",
        "    batch_size=20, shuffle=True, num_workers=5,\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset = MLPDataset(val_behavior, train=True),\n",
        "    batch_size=20, shuffle=False, num_workers=5,\n",
        ")\n",
        "\n",
        "watch_loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor([1,2,2,2,2,2,2,2,2,2]).cuda())\n",
        "share_loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLTc458XR9yz"
      },
      "source": [
        "def get_inputs(batch_data, model_type='MLP'):\n",
        "    if model_type.lower() == 'mlp':\n",
        "        feed_dict_cuda = {\n",
        "            'user_id': data['user_id'].long().cuda(),\n",
        "            'video_id': data['video_id'].long().cuda(),\n",
        "            'user_province': data['user_province'].long().cuda(),\n",
        "            'user_city': data['user_city'].long().cuda(),\n",
        "            'user_device': data['user_device'].long().cuda(),\n",
        "        }\n",
        "        return feed_dict_cuda\n",
        "    else if model_type.lower() == 'deepfm':\n",
        "        sparse_feat = torch.stack([data[x].long() for x in ['user_id', 'video_id', 'user_province', 'user_city', 'user_device']]).T\n",
        "        dense_feat = torch.stack([data[x].float() for x in ['user_age', 'city_level']]).T\n",
        "        return sparse_feat, dense_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CssygG8KPHns"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "writer = SummaryWriter('multi-task-dl')\n",
        "\n",
        "# train\n",
        "EPOCH = 10\n",
        "\n",
        "step = 0\n",
        "for epoch in range(EPOCH):\n",
        "    for idx, data in tqdm(enumerate(train_loader),total=len(train_loader), desc=f'Epoch {epoch}'):\n",
        "        # feed_dict_cuda = get_inputs(data, 'mlp')\n",
        "        sparse_feat, dense_feat = get_inputs(data, 'deepfm')\n",
        "        \n",
        "        watch_label = data['watch_label'].long().cuda()\n",
        "        share_label = data['share_label'].float().cuda().view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # watch_pred, share_pred = model(feed_dict_cuda)\n",
        "        watch_pred, share_pred = model(sparse_feat, dense_feat)\n",
        "        loss = watch_loss_fn(watch_pred, watch_label) + share_loss_fn(share_pred, share_label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc = (watch_pred.argmax(1) == watch_label).float().mean().item()\n",
        "        true_acc = ((watch_pred.argmax(1) == watch_label) & (watch_label != 0)).float().sum()\n",
        "        true_acc /= (watch_label != 0).float().sum()\n",
        "        \n",
        "        writer.add_scalar('Train/Total-ACC', total_acc, step)\n",
        "        writer.add_scalar('Train/True-ACC', true_acc, step)\n",
        "        writer.add_scalar('Train/Loss', loss.item(), step)\n",
        "        writer.flush()\n",
        "        step += 1\n",
        "        \n",
        "        # valid\n",
        "        if idx!=0 and idx % 1000 == 0:\n",
        "            val_acc = []\n",
        "            val_pred, val_label = [], []\n",
        "            with torch.no_grad():\n",
        "                for data in tqdm(val_loader, desc=f'Val'):\n",
        "                    val_label += list(data['watch_label'].data.numpy())\n",
        "\n",
        "                    # feed_dict_cuda = get_inputs(data, 'mlp')\n",
        "                    sparse_feat, dense_feat = get_inputs(data, 'deepfm')\n",
        "\n",
        "                    watch_label = data['watch_label'].long().cuda()\n",
        "                    share_label = data['share_label'].float().cuda().view(-1, 1)\n",
        "\n",
        "                    # watch_pred, share_pred = model(feed_dict_cuda)\n",
        "                    watch_pred, share_pred = model(sparse_feat, dense_feat)\n",
        "\n",
        "                    val_pred += list(watch_pred.argmax(1).data.cpu().numpy())\n",
        "                    true_acc = ((watch_pred.argmax(1) == watch_label) & (watch_label != 0)).float().sum()\n",
        "                    true_acc /= (watch_label != 0).float().sum()\n",
        "                    val_acc.append(true_acc)\n",
        "            \n",
        "            val_label = np.array(val_label)\n",
        "            val_pred = np.array(val_pred)\n",
        "\n",
        "            # the score we got\n",
        "            score = 0 \n",
        "            for aucflag, lbl in zip(np.linspace(0.1, 0.9, 9), range(1, 10)):\n",
        "                pred = (val_pred == lbl).astype(int)\n",
        "                label = (val_label == lbl).astype(int) # 注意这里是转 int , threshhold为0.5，可以自行搜索确定最优的值，但是这是在模型基本确定之后的事了\n",
        "                score += aucflag * roc_auc_score(label, pred)\n",
        "\n",
        "            val_acc = pd.DataFrame([x.item() for x in val_acc]).fillna(0).mean()[0]\n",
        "            writer.add_scalar('Val/True-ACC', val_acc, step)\n",
        "            writer.add_scalar('Val/AUC', score, step)\n",
        "            writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6HjyS8TC0kJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL-wWpuxPtUd"
      },
      "source": [
        "# predict\n",
        "# note : USE history data to complete test data inputs format.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = MLPDataset(test_data, train=False),\n",
        "    batch_size=20, shuffle=False, num_workers=5,\n",
        ")\n",
        "\n",
        "test_watch = []\n",
        "test_share = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "         # feed_dict_cuda = get_inputs(data, 'mlp')\n",
        "        sparse_feat, dense_feat = get_inputs(data, 'deepfm')\n",
        "\n",
        "        # watch_pred, share_pred = model(feed_dict_cuda)\n",
        "        watch_pred, share_pred = model(sparse_feat, dense_feat)\n",
        "        \n",
        "        test_watch += list(watch_pred.argmax(1).cpu().data.numpy())\n",
        "        test_share += list((share_pred.sigmoid() > 0.5).int().cpu().data.numpy().flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddciX71sPwtp"
      },
      "source": [
        "# write in submission.csv\n",
        "test_data['watch_label'] = test_watch\n",
        "test_data['is_share'] = test_share\n",
        "\n",
        "test_data.to_csv('submission.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Nwu1CU-kTL"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEbol-7x-j6H"
      },
      "source": [
        "test_data = pd.read_hdf('digix-data.hdf', 'test_data')\n",
        "user_features = pd.read_hdf('digix-data.hdf', 'user_features')\n",
        "video_features = pd.read_hdf('digix-data.hdf', 'video_features')\n",
        "history_behavior = pd.read_hdf('digix-data.hdf', 'history_behavior')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bucTfKjy-qCb"
      },
      "source": [
        "# simple\n",
        "history_behavior = history_behavior[history_behavior['watch_label'] != 0]\n",
        "\n",
        "user_features = user_features.set_index('user_id')\n",
        "video_features = video_features.set_index('video_id')\n",
        "\n",
        "video_features['video_release_year'] = video_features['video_release_date'].fillna('').apply(lambda x:x[:4])\n",
        "video_features = video_features[video_features['video_release_year'] != '']\n",
        "video_features['video_release_year'] = video_features['video_release_year'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL63NPhR-2G-"
      },
      "source": [
        "# split\n",
        "train_behavior = history_behavior[history_behavior['pt_d'] != 20210502]\n",
        "val_behavior = history_behavior[history_behavior['pt_d'] == 20210502]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGjgvGpr--6j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbgqTEFk-_YL"
      },
      "source": [
        "# new feature:  e.g. pt_d_COUNT\n",
        "train_user_behavior_agg = train_behavior.groupby('user_id').agg({\n",
        "    'pt_d': ['count', 'nunique'],\n",
        "    'video_id': ['nunique'],\n",
        "    'is_watch': ['mean', 'max'],\n",
        "    'is_share': ['mean', 'max'],\n",
        "    'watch_label': ['nunique', 'max']\n",
        "})\n",
        "\n",
        "train_user_behavior_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in train_user_behavior_agg.columns.tolist()])\n",
        "train_user_behavior_agg = train_user_behavior_agg.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFsu0t3RFyUT"
      },
      "source": [
        "# for predicting NEW user\n",
        "# train_user_behavior_agg['flag'] = train_user_behavior_agg['user_id'].isin(val_behavior['user_id'])\n",
        "# train_user_behavior_agg['flag'] = train_user_behavior_agg['flag'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmp4A4ohEdqb"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from itertools import combinations\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "n_fold = 5\n",
        "skf = StratifiedKFold(n_splits = n_fold, shuffle = True)\n",
        "eval_fun = roc_auc_score\n",
        "\n",
        "def run_oof(clf, X_train, y_train, X_test, kf):\n",
        "    print(clf)\n",
        "    preds_train = np.zeros((len(X_train), 2), dtype = np.float)\n",
        "    preds_test = np.zeros((len(X_test), 2), dtype = np.float)\n",
        "    train_loss = []; test_loss = []\n",
        "\n",
        "    i = 1\n",
        "    for train_index, test_index in kf.split(X_train, y_train):\n",
        "        x_tr = X_train[train_index]\n",
        "        x_te = X_train[test_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        y_te = y_train[test_index]\n",
        "        clf.fit(x_tr, y_tr, eval_set = [(x_te, y_te)], early_stopping_rounds = 500, verbose = 20)\n",
        "        \n",
        "        train_loss.append(eval_fun(y_tr, clf.predict_proba(x_tr)[:]))\n",
        "        test_loss.append(eval_fun(y_te, clf.predict_proba(x_te)[:]))\n",
        "\n",
        "        preds_train[test_index] = clf.predict_proba(x_te)[:]\n",
        "        preds_test += clf.predict_proba(X_test)[:]\n",
        "\n",
        "        # print('{0}: Train {1:0.7f} Val {2:0.7f}/{3:0.7f}'.format(i, train_loss[-1], test_loss[-1], np.mean(test_loss)))\n",
        "        print('-' * 50)\n",
        "        i += 1\n",
        "    print('Train: ', train_loss)\n",
        "    print('Val: ', test_loss)\n",
        "    print('-' * 50)\n",
        "    # print('Train{0:0.5f}_Test{1:0.5f}\\n\\n'.format(np.mean(train_loss), np.mean(test_loss)))\n",
        "    preds_test /= n_fold\n",
        "    return preds_train, preds_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZIuSKXBGZEo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe8KofkoG6md"
      },
      "source": [
        "Resource: https://sites.google.com/view/kdd20-marketplace-autorecsys/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLojojUH9Z_Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htx9PiQIHFuU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}