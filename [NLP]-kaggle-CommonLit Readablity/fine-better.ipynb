{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T16:01:24.647041Z","iopub.execute_input":"2021-08-01T16:01:24.647549Z","iopub.status.idle":"2021-08-01T16:01:24.678152Z","shell.execute_reply.started":"2021-08-01T16:01:24.647434Z","shell.execute_reply":"2021-08-01T16:01:24.67717Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torch==1.9.0","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:24.679463Z","iopub.execute_input":"2021-08-01T16:01:24.679791Z","iopub.status.idle":"2021-08-01T16:01:24.686116Z","shell.execute_reply.started":"2021-08-01T16:01:24.679759Z","shell.execute_reply":"2021-08-01T16:01:24.684569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TPU\n\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev\n# !rm -rf /kaggle/working/*.whl\n# !rm -rf /kaggle/working/pytorch-xla-env-setup.py\n# !pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:24.688344Z","iopub.execute_input":"2021-08-01T16:01:24.688673Z","iopub.status.idle":"2021-08-01T16:01:24.69858Z","shell.execute_reply.started":"2021-08-01T16:01:24.688642Z","shell.execute_reply":"2021-08-01T16:01:24.697555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom time import time\n\nimport os\nfrom pathlib import Path\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\n# import torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom torch.nn.functional import mse_loss\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup, AutoConfig, AdamW\n\ntry:\n    from torch.optim.swa_utils import AveragedModel, update_bn, SWALR\n    SWA_AVAILABLE = True\nexcept ImportError:\n    SWA_AVAILABLE = False\nfinally:\n    print(f\"SWA Available :: {SWA_AVAILABLE}\")\n    \ntry: \n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    from accelerate import Accelerator\n    XLA_AVAILABLE = True\nexcept ImportError:\n    XLA_AVAILABLE = False\nfinally:\n    print(f\"XLA AVAILABLE :: {XLA_AVAILABLE}\")\n    \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-talk')\n# print(plt.style.available)\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nby_ = Back.YELLOW\nsr_ = Style.RESET_ALL\n\nprint(\"torch: \", torch.__version__)\nprint(\"transformers: \", transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:24.700293Z","iopub.execute_input":"2021-08-01T16:01:24.700649Z","iopub.status.idle":"2021-08-01T16:01:32.458821Z","shell.execute_reply.started":"2021-08-01T16:01:24.700615Z","shell.execute_reply":"2021-08-01T16:01:32.45703Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path('./scripts').mkdir(exist_ok=True)\nmodels_dir = Path('./model')\nmodels_dir.mkdir(exist_ok=True)\n\n\n#############################################################################\nkfold_df = pd.read_csv('../input/d/racleray/cmlit-fold/train_data.csv')\n\n# Remove incomplete entries if any.\n# kfold_df.drop(kfold_df[(kfold_df.target == 0) & (kfold_df.standard_error == 0)].index, inplace=True)\n# kfold_df.reset_index(drop=True, inplace=True)\n\n\n# Other set\n# kfold_df = pd.read_csv('/kaggle/input/cmlit-fold/kfold_parsed.csv')\n# kfold_df[\"standard_error\"] = kfold_df[\"fold\"]\n\n\nprint(\"Trainset shape: \", kfold_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.460443Z","iopub.execute_input":"2021-08-01T16:01:32.460784Z","iopub.status.idle":"2021-08-01T16:01:32.587551Z","shell.execute_reply.started":"2021-08-01T16:01:32.460751Z","shell.execute_reply":"2021-08-01T16:01:32.586256Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common","metadata":{}},{"cell_type":"code","source":"class Config:\n#     model_name = 'roberta-base'\n#     pretrained_model_path = '/kaggle/input/comlitrobertabasescript/'\n    \n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/comlitrobertalargescript'\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    \n    device = 'cuda'\n    use_tpu = True\n    \n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    # 不是bert中的hidden\n    head_hidden = 512\n    \n    \n    warmup_steps = 50\n    \n    \n    #batch_size = 24\n    #eval_schedule = [(float('inf'), 10), (0.5, 4), (0.49, 3), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    #For RAM saving\n#     batch_size = 16\n#     eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    #For RAM saving\n    batch_size = 16\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 5), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    tolerance = 10\n    \n    \n    use_multi_sample_dropout = False\n    \n    # roberta base weighted model setting\n    num_hidden_layers = 12\n    layer_start = 9\n    \n    \n    # stochastic weight averaging\n    swa = False\n    swa_start = 2\n    swa_learning_rate = 2e-4\n    anneal_epochs = 1\n    anneal_strategy='cos'","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.589954Z","iopub.execute_input":"2021-08-01T16:01:32.590751Z","iopub.status.idle":"2021-08-01T16:01:32.598885Z","shell.execute_reply.started":"2021-08-01T16:01:32.590702Z","shell.execute_reply":"2021-08-01T16:01:32.598082Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=Config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.600282Z","iopub.execute_input":"2021-08-01T16:01:32.600875Z","iopub.status.idle":"2021-08-01T16:01:32.62089Z","shell.execute_reply.started":"2021-08-01T16:01:32.600833Z","shell.execute_reply":"2021-08-01T16:01:32.619888Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class RoBERTaDataset(Dataset):\n    def __init__(self, df, tokenizer, for_test=False):\n        super().__init__()\n        self.text = df['excerpt'].values\n        self.for_test = for_test\n        if not for_test:\n            self.std_err = df['standard_error'].values\n            self.target = df['target'].values\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(text,\n                                            None,\n                                            truncation=True,\n                                            add_special_tokens=True,\n                                            max_length=Config.max_len,\n                                            padding='max_length',\n                                            return_token_type_ids=True)\n\n        if not self.for_test:\n            return {\n                'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n                'std_err': torch.tensor(self.std_err[index], dtype=torch.float),\n                'label': torch.tensor(self.target[index], dtype=torch.float)\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.622649Z","iopub.execute_input":"2021-08-01T16:01:32.623322Z","iopub.status.idle":"2021-08-01T16:01:32.643923Z","shell.execute_reply.started":"2021-08-01T16:01:32.623276Z","shell.execute_reply":"2021-08-01T16:01:32.642878Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 1","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:46:25.27096Z","iopub.execute_input":"2021-07-28T08:46:25.271652Z","iopub.status.idle":"2021-07-28T08:46:25.282375Z","shell.execute_reply.started":"2021-07-28T08:46:25.271579Z","shell.execute_reply":"2021-07-28T08:46:25.278945Z"}}},{"cell_type":"code","source":"# Bad\n# class AttentionAggregation(nn.Module):\n#     def __init__(self, d_model):\n#         super().__init__()\n#         self.query = nn.Linear(d_model, 1, bias=False)\n\n#     def forward(self, x):  # (b, s, m)\n#         attns = self.query(x).softmax(dim=1)  # (b, s, 1)\n#         enc = torch.bmm(attns.transpose(1, 2), x)  # (b, 1, m)\n#         return enc.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.647051Z","iopub.execute_input":"2021-08-01T16:01:32.64773Z","iopub.status.idle":"2021-08-01T16:01:32.659559Z","shell.execute_reply.started":"2021-08-01T16:01:32.647681Z","shell.execute_reply":"2021-08-01T16:01:32.658578Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"两个都是 attn 模型，只是实现格式稍微不同","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass AttentionHead_Ori(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        \n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        \n        #context_vector = torch.bmm(attention_weights.transpose(1, 2), features)  # (b, 1, m)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self):\n        super(CLRPModel,self).__init__()\n        config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        config.update({\n                \"output_hidden_states\":True,\n                \"hidden_dropout_prob\": 0.0,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(Config.pretrained_model_path, config=config)  \n        self.head = AttentionHead_Ori(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.661936Z","iopub.execute_input":"2021-08-01T16:01:32.662527Z","iopub.status.idle":"2021-08-01T16:01:32.689194Z","shell.execute_reply.started":"2021-08-01T16:01:32.662485Z","shell.execute_reply":"2021-08-01T16:01:32.687638Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(nn.Module):\n    \"LB: 0.467 使用的模型代码\"\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.690745Z","iopub.execute_input":"2021-08-01T16:01:32.691114Z","iopub.status.idle":"2021-08-01T16:01:32.708723Z","shell.execute_reply.started":"2021-08-01T16:01:32.691079Z","shell.execute_reply":"2021-08-01T16:01:32.707481Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weighted","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start, layer_weights=None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float))\n\n    def forward(self, all_hidden_states, attention_mask):\n        all_layer_embedding = all_hidden_states[self.layer_start: ]\n        all_layer_embedding = torch.stack(all_layer_embedding, dim=0)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average\n    \n    \n    \nclass WeightedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.1,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path,\n                                                 config=self.config)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n\n        self.pooler = WeightedLayerPooling(Config.num_hidden_layers, Config.layer_start)\n\n        self.low_dropout = nn.Dropout(0.1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.regressor = nn.Linear(self.config.hidden_size, Config.num_labels)\n\n        self.std = 0.02\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(m.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## WeightedLayerPooling\n        hidden_states = outputs[2]\n        pool_out = self.pooler(hidden_states, attention_mask)[:, 0]\n        # pool_out = self.low_dropout(pool_out)\n        pool_out = self.layer_norm(pool_out)\n            \n        if Config.use_multi_sample_dropout:\n            logits = torch.mean(torch.stack([self.regressor(self.dropout(pool_out)) for _ in range(5)], dim=0), dim=0)\n        else:\n            logits = self.regressor(pool_out)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.710246Z","iopub.execute_input":"2021-08-01T16:01:32.710954Z","iopub.status.idle":"2021-08-01T16:01:32.735937Z","shell.execute_reply.started":"2021-08-01T16:01:32.710902Z","shell.execute_reply":"2021-08-01T16:01:32.734845Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean","metadata":{}},{"cell_type":"markdown","source":"##### V1: This is weaker","metadata":{}},{"cell_type":"code","source":"# V1 large :  inference model\n\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        # last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        \n        return mean_embeddings\n\n    \nclass MeanModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path,\n                                                 config=self.config)\n        \n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-7)\n        self.pooler = MeanPooling(self.config.hidden_size)\n\n        self.low_dropout = nn.Dropout(0.1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.regressor = nn.Linear(self.config.hidden_size, Config.num_labels)\n\n        self.std = 0.02\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.low_dropout(pool_out)\n        # pool_out = self.layer_norm(pool_out)\n\n        if Config.use_multi_sample_dropout:  # Don`t help\n            logits = torch.mean(torch.stack([self.regressor(self.dropout(pool_out)) for _ in range(5)], dim=0), dim=0)\n        else:\n            logits = self.regressor(pool_out)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.737435Z","iopub.execute_input":"2021-08-01T16:01:32.737952Z","iopub.status.idle":"2021-08-01T16:01:32.75792Z","shell.execute_reply.started":"2021-08-01T16:01:32.737911Z","shell.execute_reply":"2021-08-01T16:01:32.756601Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanModelEmbedding(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path,\n                                                 config=self.config)\n        \n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-7)\n        self.pooler = MeanPooling(self.config.hidden_size)\n\n        self.low_dropout = nn.Dropout(0.1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.regressor = nn.Linear(self.config.hidden_size, Config.num_labels)\n\n        self.std = 0.02\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.low_dropout(pool_out)\n\n        return pool_out","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.759531Z","iopub.execute_input":"2021-08-01T16:01:32.759864Z","iopub.status.idle":"2021-08-01T16:01:32.781629Z","shell.execute_reply.started":"2021-08-01T16:01:32.759832Z","shell.execute_reply":"2021-08-01T16:01:32.780582Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### V2","metadata":{}},{"cell_type":"code","source":"# V2 large :  inference model\n\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        # last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        \n        return mean_embeddings\n\n\n\n\n#Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n#https://arxiv.org/abs/1908.08681v1\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n\n\n\n# def gelu(x):\n#     \"\"\"Implementation of the gelu activation function.\n#             For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n#             0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n#     \"\"\"\n#     return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2)))\n\n\n# class GeLU(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n\n#     def forward(self, x):\n#         return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2)))\n    \n    \n    \nclass MeanModel_v2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path,\n                                                 config=self.config)\n        \n        self.pooler = MeanPooling(self.config.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        \n        self.dropout = nn.Dropout(0.5)\n        self.regressor = nn.Sequential(\n                nn.Linear(self.config.hidden_size, 512),\n                Mish(),\n                nn.Linear(512, 1)\n        )\n        \n        self.std = 0.02\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.dropout(pool_out)\n        pool_out = self.layer_norm(pool_out)\n        logits = self.regressor(pool_out)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.784146Z","iopub.execute_input":"2021-08-01T16:01:32.78457Z","iopub.status.idle":"2021-08-01T16:01:32.80585Z","shell.execute_reply.started":"2021-08-01T16:01:32.784517Z","shell.execute_reply":"2021-08-01T16:01:32.804741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AttModel","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n        self.std = 0.02\n#         self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=self.std)\n                # init.kaiming_normal_(m.weight, mode='fan_in')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, hidden_state, attention_mask):\n\n        att = torch.tanh(self.W(hidden_state))\n        score = self.V(att)\n\n        mask_expanded = attention_mask.unsqueeze(-1)\n        score[~mask_expanded] = -1e9\n\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * hidden_state\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass AttModel(nn.Module):\n    def __init__(self, attn_type='tradition'):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(Config.pretrained_model_path,\n                                                 config=self.config)\n        self.head = AttentionHead(self.config.hidden_size, Config.head_hidden)\n        \n        self.dropout = nn.Dropout(p=0.1)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n\n        self.regressor = nn.Linear(self.config.hidden_size, Config.num_labels)\n#         self.regressor = nn.Sequential(\n#                 nn.Linear(self.config.hidden_size, 512),\n#                 Mish(),\n#                 nn.Linear(512, 1)\n#         )\n        \n        self.m_dropout = nn.Dropout(p=0.5)\n\n        self.std = 0.02\n#         self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.std)\n            # init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs[2][-1]\n\n        x = self.head(hidden_states, attention_mask)\n#         x = self.dropout(x)\n#         x = self.layer_norm(x)\n        logits = self.regressor(x)\n#         logits = torch.mean(torch.stack([self.regressor(self.m_dropout(x)) for _ in range(5)], dim=0), dim=0)\n\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.807659Z","iopub.execute_input":"2021-08-01T16:01:32.808385Z","iopub.status.idle":"2021-08-01T16:01:32.830127Z","shell.execute_reply.started":"2021-08-01T16:01:32.808337Z","shell.execute_reply":"2021-08-01T16:01:32.8291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optim","metadata":{}},{"cell_type":"code","source":"# def create_optimizer(model, train_loader_len):\n#     named_parameters = list(model.named_parameters())    \n    \n#     roberta_parameters = named_parameters[:197]    \n#     attention_parameters = named_parameters[199:203]\n#     regressor_parameters = named_parameters[203:]\n        \n#     attention_group = [params for (name, params) in attention_parameters]\n#     regressor_group = [params for (name, params) in regressor_parameters]\n\n#     parameters = []\n#     parameters.append({\"params\": attention_group})\n#     parameters.append({\"params\": regressor_group})\n\n#     for layer_num, (name, params) in enumerate(roberta_parameters):\n#         weight_decay = 0.0 if \"bias\" in name else 0.01\n\n#         lr = Config.lr\n\n#         if layer_num >= 69:        \n#             lr = Config.lr * 2.5\n\n#         if layer_num >= 133:\n#             lr = Config.lr * 5\n\n#         parameters.append({\"params\": params,\n#                            \"weight_decay\": weight_decay,\n#                            \"lr\": lr})\n        \n#     max_train_steps = Config.epochs * train_loader_len\n#     warmup_steps = Config.warmup_steps if Config.warmup_steps >= 0 else math.ceil(max_train_steps * 0.05)\n#     # print(\">>Config max_train_steps: \", max_train_steps)\n#     # print(\">>Config warmup_steps: \", warmup_steps)\n    \n#     optimizer = optim.AdamW(parameters)\n#     # Defining LR Scheduler\n#     scheduler = get_cosine_schedule_with_warmup(\n#         optimizer,\n#         num_warmup_steps=warmup_steps,\n#         num_training_steps=max_train_steps\n#     )\n\n#     return optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.83194Z","iopub.execute_input":"2021-08-01T16:01:32.832729Z","iopub.status.idle":"2021-08-01T16:01:32.851274Z","shell.execute_reply.started":"2021-08-01T16:01:32.83268Z","shell.execute_reply":"2021-08-01T16:01:32.850043Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model, train_loader_len, custom_multiples=50, group_diff=1.5):\n    # base 2.5\n    # large 1.5\n    \n    model_params = list(model.named_parameters())    \n\n    no_decay = [\"bias\", \"gamma\", \"beta\", \"LayerNorm.weight\"]\n    \n#     group1=['embeddings', 'layer.0.','layer.1.','layer.2.','layer.3.']\n#     group2=['layer.4.','layer.5.','layer.6.','layer.7.']\n#     group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    \n    group1=['embeddings', 'layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.']\n    group2=['layer.8.','layer.9.','layer.10.','layer.11.','layer.12.','layer.13.','layer.14.','layer.15.']\n    group3=['layer.16.','layer.17.','layer.18.','layer.19.','layer.20.','layer.21.','layer.22.','layer.23.']\n    \n    group_all = group1 + group2 + group3\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model_params if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n         'weight_decay': Config.weight_decay,\n         'lr': Config.lr * custom_multiples},\n        \n        {'params': [p for n, p in model_params if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n         'weight_decay': Config.weight_decay,\n         'lr': Config.lr * group_diff},\n        \n        {'params': [p for n, p in model_params if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n         'weight_decay': Config.weight_decay,\n         'lr': Config.lr},\n        \n        {'params': [p for n, p in model_params if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n         'weight_decay': Config.weight_decay,\n         'lr': Config.lr * group_diff * group_diff},\n        \n        {'params': [p for n, p in model_params if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],\n         'weight_decay': 0.0,\n         'lr': Config.lr * custom_multiples},\n        \n        {'params': [p for n, p in model_params if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],\n         'weight_decay': 0.0,\n         'lr': Config.lr * group_diff},\n        \n        {'params': [p for n, p in model_params if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],\n         'weight_decay': 0.0,\n         'lr': Config.lr},\n        \n        {'params': [p for n, p in model_params if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],\n         'weight_decay': 0.0,\n         'lr': Config.lr * group_diff * group_diff},\n    ]\n\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=Config.lr)\n\n    max_train_steps = Config.epochs * train_loader_len\n\n    warmup_steps = Config.warmup_steps if Config.warmup_steps >= 0 else math.ceil(max_train_steps * 0.05)\n    # print(\">>Config max_train_steps: \", max_train_steps)\n    # print(\">>Config warmup_steps: \", warmup_steps)\n    \n    # Defining LR Scheduler\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=max_train_steps\n    )\n    \n    return optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.852543Z","iopub.execute_input":"2021-08-01T16:01:32.855349Z","iopub.status.idle":"2021-08-01T16:01:32.877329Z","shell.execute_reply.started":"2021-08-01T16:01:32.855281Z","shell.execute_reply":"2021-08-01T16:01:32.876035Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EvaluationScheduler","metadata":{}},{"cell_type":"code","source":"import math\n\nclass AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return math.sqrt(self.loss / self.n_samples) # rmse\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n        ## for less evaluation\n        # if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n        #     self.penalty += self.penalize_factor\n        #     self.penalty = min(self.penalty, self.max_penalty)\n        #     self.evaluation_interval += self.penalty\n        # else:\n        #     self.penalty = 0\n            \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.879168Z","iopub.execute_input":"2021-08-01T16:01:32.879546Z","iopub.status.idle":"2021-08-01T16:01:32.897474Z","shell.execute_reply.started":"2021-08-01T16:01:32.879504Z","shell.execute_reply":"2021-08-01T16:01:32.89626Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"class FGM():\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n\n    def attack(self, epsilon=0.001, emb_name='word_embeddings.'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0:\n                    r_at = epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self, emb_name='word_embeddings.'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name: \n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.899183Z","iopub.execute_input":"2021-08-01T16:01:32.899604Z","iopub.status.idle":"2021-08-01T16:01:32.91846Z","shell.execute_reply.started":"2021-08-01T16:01:32.899566Z","shell.execute_reply":"2021-08-01T16:01:32.917212Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mse_loss(y_true, y_pred, standard_error=None):\n    return nn.functional.mse_loss(y_true, y_pred)\n\n\ndef gll_loss(y_true, y_pred, standard_error):\n    # crit = torch.nn.GaussianNLLLoss()\n    y_pred = y_pred.view(-1)\n    y_true = y_true.view(-1)\n    standard_error = standard_error.view(-1)\n    loss = torch.nn.GaussianNLLLoss()(input=y_pred,\n                                     target=y_true, \n                                     var=standard_error ** 2)\n    return loss\n\n\ndef metrics(y_true, y_pred):\n    return nn.functional.mse_loss(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.920132Z","iopub.execute_input":"2021-08-01T16:01:32.920619Z","iopub.status.idle":"2021-08-01T16:01:32.935836Z","shell.execute_reply.started":"2021-08-01T16:01:32.920565Z","shell.execute_reply":"2021-08-01T16:01:32.934755Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, metrics, model_num,\n                 use_gll=False, use_fgm=False, \n                 swa_model=None, swa_scheduler=None,\n                 use_tpu=False, device=\"cuda\"):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        \n        self.use_fgm = use_fgm\n        if use_fgm:\n            self.fgm = FGM(self.model)\n        self.use_swa = False\n        if swa_model:\n            self.use_swa = True\n            self.swa_model = swa_model\n            self.swa_scheduler = swa_scheduler\n        \n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        self.use_gll = use_gll\n        self.metrics = metrics\n        self.model_num = model_num\n        \n        self.use_tpu = use_tpu\n        if use_tpu and XLA_AVAILABLE:\n            xm.master_print(f\">>> {device} is used\")\n            \n    \n    def log(self, string):\n        if self.use_tpu:\n            xm.master_print(string)\n        else:\n            print(string)\n    \n    \n    def save(self):\n        if self.use_tpu:\n            xm.save(self.model.state_dict(), models_dir / f'model_{self.model_num}.bin')\n            self.model.config.to_json_file(models_dir / 'config.json', use_diff=True)\n        else:\n            torch.save(self.model.state_dict(), models_dir / f'model_{self.model_num}.bin')\n            self.model.config.to_json_file(models_dir / 'config.json', use_diff=True)\n             \n                \n    def run(self, begin_val_epoch=3):\n        record_info = {'train_loss': [], 'val_loss': [], 'swa_loss': []}\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0            \n        \n        for epoch in range(Config.epochs):\n            self.log(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n    \n            start_epoch_time = time()\n            tolerance = 0\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train_step(batch, epoch)\n                # self.log(f'{epoch+1}#[{step+1}/{len(self.train_dl)}]: train loss - {train_loss}')\n\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss))\n                \n                # check if valid by setting eval_schedule\n                if (epoch + 1) >= begin_val_epoch and evaluation_scheduler.step(step):  \n                    tolerance += 1\n                    val_loss = self.evaluate()\n                    record_info['val_loss'].append((step, val_loss))\n                    self.log(f'\\t\\t{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg():.4f} | val loss - {val_loss:.4f}')\n                    \n                    train_loss_counter.reset()\n                    if val_loss < best_val_loss:\n                        tolerance = 0\n                        self.log(f\"\\t\\t{g_}Val loss decreased from {best_val_loss:.4f} to {val_loss:.4f}{sr_}\")\n                        best_val_loss = val_loss\n                        if best_val_loss < 0.495:\n                            self.save()\n                        \n#                         ########################## testing\n#                         if not self.use_swa:\n#                             self.scheduler.step()\n#                         else:\n#                             if (epoch+1) < Config.swa_start:\n#                                 self.scheduler.step()\n#                         ########################## testing\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss)\n                    \n                \n                if (epoch + 1) == Config.epochs and tolerance > Config.tolerance:\n                    self.log(f\"\\t\\t{r_}Val loss stable, break. Cur best loss {best_val_loss:.4f} {sr_}\")\n                    break\n\n                step += 1\n                \n            gc.collect()\n            \n            if self.use_swa and (epoch+1) >= Config.swa_start:\n                self.swa_model.update_parameters(self.model)\n                self.swa_scheduler.step()\n                val_loss_s = self.swa_evaluate()\n                record_info['swa_loss'].append((step, val_loss_s))\n                \n            end_epoch_time = time()\n            self.log(f'{bb_}{y_}The epoch took {end_epoch_time - start_epoch_time:.4f} sec {sr_}')\n            \n        if self.use_swa:\n            update_bn(self.train_dl, self.swa_model, device=torch.device('cuda'))    \n        \n        return record_info, best_val_loss\n            \n    def train_step(self, batch, epoch):\n        self.model.train()\n        sent_id, mask, std_err, labels = batch['input_ids'].to(self.device), \\\n                                         batch['attention_mask'].to(self.device), \\\n                                         batch['std_err'].to(self.device), \\\n                                         batch['label'].to(self.device)\n        \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(labels.unsqueeze(1), preds, std_err)\n        train_loss.backward(retain_graph=self.use_fgm)\n        \n        if self.use_fgm:\n            self.fgm.attack()\n            preds_adv = self.model(sent_id, mask)\n            adv_loss = self.criterion(labels.unsqueeze(1), preds, std_err)\n            adv_loss.backward()\n            self.fgm.restore()\n        \n        self.optimizer.step()\n        if not self.use_swa:\n            self.scheduler.step()\n        else:\n            if (epoch+1) < Config.swa_start:\n                self.scheduler.step()\n        \n        if self.use_gll:\n            return self.metrics(labels.unsqueeze(1), preds).item()\n        else:\n            return train_loss.item()\n\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step, batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), \\\n                                    batch['attention_mask'].to(self.device), \\\n                                    batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                \n                if self.use_gll:\n                    val_loss_counter.update(self.metrics(labels.unsqueeze(1), preds).item(), len(labels))\n                else:\n                    loss = self.criterion(labels.unsqueeze(1), preds, None)\n                    val_loss_counter.update(loss.item(), len(labels))\n                \n        return val_loss_counter.avg()\n    \n    \n    def swa_evaluate(self):\n        self.swa_model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step, batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), \\\n                                    batch['attention_mask'].to(self.device), \\\n                                    batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.swa_model(sent_id, mask)\n                \n                if self.use_gll:\n                    val_loss_counter.update(self.metrics(labels.unsqueeze(1), preds).item(), len(labels))\n                else:\n                    loss = self.criterion(labels.unsqueeze(1), preds, None)\n                    val_loss_counter.update(loss.item(), len(labels))\n                \n        return val_loss_counter.avg()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:03:04.799858Z","iopub.execute_input":"2021-08-01T16:03:04.800296Z","iopub.status.idle":"2021-08-01T16:03:04.83713Z","shell.execute_reply.started":"2021-08-01T16:03:04.800255Z","shell.execute_reply":"2021-08-01T16:03:04.83603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_data(fold, df, tokenizer):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    # 使用pl TPU 效率变低，因为pl会使用sampler_ddp优化对TPU的训练\n    # train_dataset = SmartBatchingDataset(df_train)\n    # tokenizer = AutoTokenizer.from_pretrained(Config.tokenizer)\n    # train_loader = train_dataset.get_dataloader(batch_size=Config.batch_size,\n    #                                             max_len=Config.max_len,\n    #                                             pad_id=tokenizer.pad_token_id)\n\n    train_dataset = RoBERTaDataset(df_train, tokenizer)\n    sampler = RandomSampler(train_dataset)\n    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size,\n                              num_workers=4, sampler=sampler, pin_memory=True)\n\n    valid_dataset = RoBERTaDataset(df_valid, tokenizer)\n    valid_loader = DataLoader(valid_dataset, batch_size=Config.batch_size,\n                              num_workers=4, shuffle=False, pin_memory=True)\n\n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.977506Z","iopub.execute_input":"2021-08-01T16:01:32.978123Z","iopub.status.idle":"2021-08-01T16:01:32.997842Z","shell.execute_reply.started":"2021-08-01T16:01:32.977969Z","shell.execute_reply":"2021-08-01T16:01:32.996929Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preditc","metadata":{}},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:32.999274Z","iopub.execute_input":"2021-08-01T16:01:32.999788Z","iopub.status.idle":"2021-08-01T16:01:33.01539Z","shell.execute_reply.started":"2021-08-01T16:01:32.999736Z","shell.execute_reply":"2021-08-01T16:01:33.014409Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reparams(model, reinit_layers=4):\n    # TF version uses truncated_normal for initialization. This is Pytorch\n    if reinit_layers > 0:\n        print(f'Reinitializing Last {reinit_layers} Layers ...')\n        encoder_temp = getattr(model, \"roberta\")\n        for layer in encoder_temp.encoder.layer[-reinit_layers: ]:\n            for module in layer.modules():\n                if isinstance(module, nn.Linear):\n                    module.weight.data.normal_(mean=0.0, std=0.02)\n                    if module.bias is not None:\n                        module.bias.data.zero_()\n                elif isinstance(module, nn.Embedding):\n                    module.weight.data.normal_(mean=0.0, std=0.02)\n                    if module.padding_idx is not None:\n                        module.weight.data[module.padding_idx].zero_()\n                elif isinstance(module, nn.LayerNorm):\n                    module.bias.data.zero_()\n                    module.weight.data.fill_(1.0)\n        print('Reinitializing Done.!')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:33.016792Z","iopub.execute_input":"2021-08-01T16:01:33.017338Z","iopub.status.idle":"2021-08-01T16:01:33.030844Z","shell.execute_reply.started":"2021-08-01T16:01:33.017294Z","shell.execute_reply":"2021-08-01T16:01:33.029287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{}},{"cell_type":"code","source":"def run(fold_init=0, fold_end=5):\n    best_scores = []\n    records = []\n    tokenizer = AutoTokenizer.from_pretrained(Config.pretrained_model_path)\n    tokenizer.save_pretrained(models_dir)\n    \n    for fold_num in range(fold_init, fold_end): \n        if Config.use_tpu and XLA_AVAILABLE:\n            accelerator = Accelerator()\n            xm.master_print(f'{by_}{r_}  Model#{fold_num+1}  {sr_}')\n            device = accelerator.device\n        else:\n            print(f'{by_}{r_}  Model#{fold_num+1}  {sr_}')\n            device = Config.device\n        \n\n        # seed_everything(Config.seed + fold) \n\n        train_dl, val_dl = get_train_data(fold_num, kfold_df, tokenizer)\n        \n        #########################################################\n        ###### Change Model\n        #########################################################\n        model = AttModel()\n        # model = CLRPModel()\n        # model = MeanModel()\n        # model = WeightedModel()\n        # model = MeanModel_v2()\n        model = model.to(device)\n        \n        reparams(model, reinit_layers=2)\n        \n        optimizer, scheduler = create_optimizer(model, len(train_dl))\n        \n        if Config.swa:\n            # stochastic weight averaging\n            swa_model = AveragedModel(model)\n            swa_scheduler = SWALR(\n                optimizer, swa_lr=Config.swa_learning_rate, \n                anneal_epochs=Config.anneal_epochs, \n                anneal_strategy=Config.anneal_strategy\n            )\n        \n        if Config.use_tpu and XLA_AVAILABLE:\n            model, train_dl, val_dl, optimizer, scheduler = accelerator.prepare(\n                              model, train_dl, val_dl, optimizer, scheduler)\n            \n            # gll not work\n#             trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, \n#                               gll_loss, metrics, fold_num, use_gll=True, \n#                               use_tpu=True, device=device)\n\n            trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, \n                              mse_loss, metrics, fold_num, use_gll=False, \n                              use_tpu=True, device=device)\n            \n            # fgm\n#             trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, \n#                               mse_loss, metrics, fold_num, use_gll=False, \n#                               use_tpu=True, use_fgm=True, device=device)\n\n        else:\n            trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, \n                              mse_loss, metrics, fold_num, use_gll=False)\n            # pytorch 1.9\n#             trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, \n#                               gll_loss, metrics, fold_num, use_gll=True)\n\n############################################################################\n# For memory saving\n        if fold_num < 3:\n            record_info, best_val_loss = trainer.run(begin_val_epoch=4)\n        else:\n            record_info, best_val_loss = trainer.run(begin_val_epoch=3)\n############################################################################\n\n        #  record_info, best_val_loss = trainer.run(begin_val_epoch=3)\n        best_scores.append(best_val_loss)  \n        records.append(record_info)\n        \n        if not Config.use_tpu:\n            steps, train_losses = list(zip(*record_info['train_loss']))\n            plt.plot(steps, train_losses, label='train_loss')\n            steps, val_losses = list(zip(*record_info['val_loss']))\n            plt.plot(steps, val_losses, label='val_loss')\n            plt.legend()\n            plt.show()\n            \n        del model\n        gc.collect()\n        # torch.cuda.empty_cache()\n\n    \n    if Config.use_tpu and XLA_AVAILABLE:\n        xm.master_print(f'Best val losses:, ',  best_scores)\n        xm.master_print(f'Avg val loss: {np.array(best_scores).mean():.4f}')        \n    else:\n        print(f'Best val losses:, ',  best_scores)\n        print(f'Avg val loss: {np.array(best_scores).mean():.4f}')\n    \n    return best_scores, records\n\n# !date '+%A %W %Y %X' > execution_time","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:33.034367Z","iopub.execute_input":"2021-08-01T16:01:33.034875Z","iopub.status.idle":"2021-08-01T16:01:33.055077Z","shell.execute_reply.started":"2021-08-01T16:01:33.034833Z","shell.execute_reply":"2021-08-01T16:01:33.05386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n#     model_name = 'roberta-base'\n#     pretrained_model_path = '/kaggle/input/comlitrobertabasescript/'\n    \n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/comlitrobertalargescript'\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    \n    device = 'cuda'\n    use_tpu = True\n    \n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    # 不是bert中的hidden\n    head_hidden = 512\n    \n    \n    warmup_steps = 50\n    \n    \n    #batch_size = 24\n    #eval_schedule = [(float('inf'), 10), (0.5, 4), (0.49, 3), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    #For RAM saving\n#     batch_size = 16\n#     eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    #For RAM saving\n    batch_size = 16\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 5), (0.48, 2), (0.47, 1), (0, 0)]\n    \n    tolerance = 10\n    \n    \n    use_multi_sample_dropout = False\n    \n    # roberta base weighted model setting\n    num_hidden_layers = 12\n    layer_start = 9\n    \n    \n    # stochastic weight averaging\n    swa = False\n    swa_start = 2\n    swa_learning_rate = 2e-4\n    anneal_epochs = 1\n    anneal_strategy='cos'","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:33.057036Z","iopub.execute_input":"2021-08-01T16:01:33.057731Z","iopub.status.idle":"2021-08-01T16:01:33.071484Z","shell.execute_reply.started":"2021-08-01T16:01:33.057674Z","shell.execute_reply":"2021-08-01T16:01:33.070587Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:01:44.10365Z","iopub.execute_input":"2021-08-01T16:01:44.104094Z","iopub.status.idle":"2021-08-01T16:01:44.325978Z","shell.execute_reply.started":"2021-08-01T16:01:44.104054Z","shell.execute_reply":"2021-08-01T16:01:44.324601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # del model\n    # torch.cuda.synchronize()\n    gc.collect()\n    # torch.cuda.empty_cache()\n    # torch.cuda.synchronize()\n    best_scores, records = run(0, 5)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:03:17.506477Z","iopub.execute_input":"2021-08-01T16:03:17.506889Z","iopub.status.idle":"2021-08-01T16:33:00.213223Z","shell.execute_reply.started":"2021-08-01T16:03:17.506849Z","shell.execute_reply":"2021-08-01T16:33:00.211846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_records(records, index):\n    record_info = records[index]\n    \n    steps, train_losses = list(zip(*record_info['train_loss']))\n    plt.plot(steps, train_losses, label='train_loss')\n    steps, val_losses = list(zip(*record_info['val_loss']))\n    plt.plot(steps, val_losses, label='val_loss')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show_records(records[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVR Stack","metadata":{}},{"cell_type":"code","source":"def get_test_data(df):\n    tokenizer = AutoTokenizer.from_pretrained(Config.pretrained_model_path)\n    test_dataset = RoBERTaDataset(df, tokenizer, for_test=True)\n    test_loader = DataLoader(test_dataset, batch_size=24,\n                             num_workers=4, shuffle=False, pin_memory=True,\n                             drop_last=False)\n    return test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rmse_score SVR\nfrom joblib import dump, load\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\n\ntrain_data = kfold_df\n\n## Reset bins\n# num_bins = int(np.floor(1 + np.log2(len(train_data))))\n# train_data.loc[:, 'bins'] = pd.cut(train_data['target'], bins=num_bins, labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\n# input whole train_data not shuffled\ndef embedding_svr_train(df, bins, save_dir, bert_path, bert_nums=5, svr_nfolds=10, C=8, kernel='rbf'):\n    mean_scores = []\n    records = []\n\n    # get embeddings\n    models_embedding = []\n    for fold_num in range(bert_nums):\n        print(f'{by_}{r_}  Model#{fold_num+1} inferencing {sr_}')\n        device = Config.device\n\n        test_dataloader = get_test_data(df)\n\n        model = MeanModelEmbedding()\n        model.load_state_dict(torch.load(bert_path + f'model_{fold_num}.bin'))\n        model.to(device)\n        model.eval()\n\n        embeddings = []\n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(test_dataloader)):\n                sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n                outputs = model(sent_id, mask)\n                outputs = outputs.detach().cpu().numpy()\n                embeddings.extend(outputs)\n            embeddings = np.array(embeddings)\n        models_embedding.append(embeddings)\n\n        del model\n        gc.collect()\n        #torch.cuda.empty_cache()\n\n    #print(models_embedding[0].shape)\n    #print(type(models_embedding[0]))    \n    print(\"Embedding got.\")    \n\n    \n    # SVM training: 5 SVR model\n    for index, X in enumerate(models_embedding):\n        print(f'{by_}{r_}  SVR#{index+1} training {sr_}')\n        scores = []\n        # new kfold\n        kfold = StratifiedKFold(n_splits=svr_nfolds, shuffle=True, random_state=42)\n        for i, (train_idx, valid_idx) in tqdm(enumerate(kfold.split(X, bins))):\n            model = SVR(C=C, kernel=kernel, gamma='auto')\n            X_train, y_train = X[train_idx], target[train_idx]\n            X_valid, y_valid = X[valid_idx], target[valid_idx]\n            \n            model.fit(X_train, y_train)\n            \n            prediction = model.predict(X_valid)\n            score = rmse_score(prediction, y_valid)\n            scores.append(score)\n            print(f'\\t\\t{y_}SVR {index} Fold {i} , rmse score: {score:.4f} {sr_}')\n\n            os.makedirs(save_dir, exist_ok=True)\n            dump(model, save_dir + f'svr_{index}_{i}.bin')\n\n        mean_score = np.mean(scores)\n        print(f'\\t{r_}SVR {index} mean rmse score: {mean_score:.4f} {sr_}')\n        mean_scores.append(mean_score)\n        records.append(scores)\n\n    print(f'Avg rmse score of 5 SVR: {np.mean(mean_scores):.4f}')\n\n    return records","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"records = embedding_svr_train(train_data, bins, save_dir=models_dir, bert_path=\"../input/clrobertalarger/\", bert_nums=5, svr_nfolds=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"过拟合严重，需要bagging","metadata":{}},{"cell_type":"code","source":"def embedding_svr_test(df, save_dir, bert_path, bert_nums=5, svr_nfolds=10):\n    # get embeddings\n    models_embedding = []\n    for fold_num in range(bert_nums):\n        print(f'{by_}{r_}  Model#{fold_num+1} inferencing {sr_}')\n        device = Config.device\n\n        test_dataloader = get_test_data(df)\n\n        model = MeanModelEmbedding()\n        model.load_state_dict(torch.load(bert_path + f'model_{fold_num}.bin'))\n        model.to(device)\n        model.eval()\n\n        embeddings = []\n        with torch.no_grad():\n            for i, batch in tqdm(enumerate(test_dataloader)):\n                sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n                outputs = model(sent_id, mask)\n                outputs = outputs.detach().cpu().numpy()\n                embeddings.extend(outputs)\n            embeddings = np.array(embeddings)\n        models_embedding.append(embeddings)\n\n        del model\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    print(f'Embedding got.')\n\n    # SVM predict: 5 SVR model\n    results = np.zeros((df.shape[0]))\n    for index, X_test in enumerate(models_embedding):\n        print(f'{by_}{r_}  SVR#{index+1} predicting {sr_}')\n        for i in range(svr_nfolds):\n            svr = load(save_dir + f'svr_{index}_{i}.bin')\n            preds = svr.predict(X_test)\n            results += preds\n            \n    print(f'Complete.')\n\n    return results / bert_nums / svr_nfolds","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda x: x.replace('\\n',' '))\nembedding_svr_test(df=test_df, save_dir=models_dir, bert_path=\"../input/clrobertalarger/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CV res test in whole trainset","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_data(df):\n    tokenizer = AutoTokenizer.from_pretrained(Config.pretrained_model_path)\n    test_dataset = RoBERTaDataset(df, tokenizer, for_test=True)\n    test_loader = DataLoader(test_dataset, batch_size=24,\n                             num_workers=4, shuffle=False, pin_memory=True,\n                             drop_last=False)\n    return test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_in_origin_data(test_df):\n    models_folder_path = Path('/kaggle/input/commonlit-roberta-0467/')\n    models_preds = []\n    n_models = 5\n    \n    for model_num in range(n_models):\n        print(f'Inference # {model_num+1}/{n_models} ...', end=' ')\n        test_dataloader = get_test_data(test_df)\n        \n        # My save style used\n        # model = torch.load(models_folder_path / f'best_model_{model_num}.pt').to(Config.device)\n        \n        # Litmodel\n        model_path = models_folder_path / f'model_{model_num+1}.pth'\n        model = LitModel()\n        model.load_state_dict(torch.load(model_path, map_location=Config.device))    \n        model.to(Config.device)\n\n        all_preds = []\n        model.eval()\n\n        for step, batch in enumerate(test_dataloader):\n            sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n            with torch.no_grad():\n                preds = model(sent_id, mask)\n                all_preds += preds.flatten().cpu().tolist()\n\n        models_preds.append(all_preds)\n        \n        print(' Completed')\n        \n    return models_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df = pd.read_csv('../input/cmlit-fold/train_data.csv')\n\n# models_preds = check_in_origin_data(test_df)\n\n# models_preds = np.array(models_preds)\n# all_preds = models_preds.mean(axis=0)\n\n# rmse_score(test_df.target.values, all_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- commonlit-roberta-0467： 0.2924174434645643","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Upload model to cloud","metadata":{}},{"cell_type":"code","source":"!ls model","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:35:17.003582Z","iopub.execute_input":"2021-08-01T16:35:17.00433Z","iopub.status.idle":"2021-08-01T16:35:18.380432Z","shell.execute_reply.started":"2021-08-01T16:35:17.004268Z","shell.execute_reply":"2021-08-01T16:35:18.37926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# !pip install kaggle\n\nos.environ[\"KAGGLE_USERNAME\"] = \"\"\nos.environ[\"KAGGLE_KEY\"] = \"\"\n\n!kaggle datasets metadata racleray/attlargereinit\n!mv dataset-metadata.json model/\n\n# 最好不要有文件夹\n!kaggle datasets version -p ./model -m \"Updated data base fine\"","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:35:24.02221Z","iopub.execute_input":"2021-08-01T16:35:24.022665Z","iopub.status.idle":"2021-08-01T16:37:09.814924Z","shell.execute_reply.started":"2021-08-01T16:35:24.022626Z","shell.execute_reply":"2021-08-01T16:37:09.813479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}