{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T04:26:25.604253Z","iopub.execute_input":"2021-08-02T04:26:25.604672Z","iopub.status.idle":"2021-08-02T04:26:25.632108Z","shell.execute_reply.started":"2021-08-02T04:26:25.604642Z","shell.execute_reply":"2021-08-02T04:26:25.630850Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/input/meanlargereinit/config.json\n/kaggle/input/meanlargereinit/merges.txt\n/kaggle/input/meanlargereinit/model_0.bin\n/kaggle/input/meanlargereinit/model_1.bin\n/kaggle/input/meanlargereinit/tokenizer.json\n/kaggle/input/meanlargereinit/vocab.json\n/kaggle/input/meanlargereinit/tokenizer_config.json\n/kaggle/input/meanlargereinit/model_3.bin\n/kaggle/input/meanlargereinit/model_2.bin\n/kaggle/input/meanlargereinit/model_4.bin\n/kaggle/input/meanlargereinit/special_tokens_map.json\n/kaggle/input/largeattnlit/config.json\n/kaggle/input/largeattnlit/merges.txt\n/kaggle/input/largeattnlit/model_0.bin\n/kaggle/input/largeattnlit/model_1.bin\n/kaggle/input/largeattnlit/tokenizer.json\n/kaggle/input/largeattnlit/vocab.json\n/kaggle/input/largeattnlit/tokenizer_config.json\n/kaggle/input/largeattnlit/model_3.bin\n/kaggle/input/largeattnlit/model_2.bin\n/kaggle/input/largeattnlit/model_4.bin\n/kaggle/input/largeattnlit/special_tokens_map.json\n/kaggle/input/pretrained/roberta_tk.pt\n/kaggle/input/pretrained/bert_cased_tk.pt\n/kaggle/input/pretrained/large_s/config.json\n/kaggle/input/pretrained/large_s/merges.txt\n/kaggle/input/pretrained/large_s/tokenizer.json\n/kaggle/input/pretrained/large_s/vocab.json\n/kaggle/input/pretrained/large_s/tokenizer_config.json\n/kaggle/input/pretrained/large_s/pytorch_model.bin\n/kaggle/input/pretrained/large_s/special_tokens_map.json\n/kaggle/input/pretrained/base_s/pre_config.json.backup\n/kaggle/input/pretrained/base_s/config.json\n/kaggle/input/pretrained/base_s/merges.txt\n/kaggle/input/pretrained/base_s/tokenizer.json\n/kaggle/input/pretrained/base_s/vocab.json\n/kaggle/input/pretrained/base_s/tokenizer_config.json\n/kaggle/input/pretrained/base_s/pytorch_model.bin\n/kaggle/input/pretrained/base_s/special_tokens_map.json\n/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n/kaggle/input/commonlitreadabilityprize/train.csv\n/kaggle/input/commonlitreadabilityprize/test.csv\n/kaggle/input/comlitothers/model_5.bin\n/kaggle/input/comlitothers/model_1.bin\n/kaggle/input/comlitothers/model_3.bin\n/kaggle/input/comlitothers/model_2.bin\n/kaggle/input/comlitothers/model_4.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom joblib import dump, load\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom time import time\nfrom colorama import Fore, Back, Style\n\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nby_ = Back.YELLOW\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.633671Z","iopub.execute_input":"2021-08-02T04:26:25.634130Z","iopub.status.idle":"2021-08-02T04:26:25.646464Z","shell.execute_reply.started":"2021-08-02T04:26:25.634088Z","shell.execute_reply":"2021-08-02T04:26:25.644949Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global config","metadata":{}},{"cell_type":"code","source":"class Config:\n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    num_labels = 1","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.649424Z","iopub.execute_input":"2021-08-02T04:26:25.650018Z","iopub.status.idle":"2021-08-02T04:26:25.657395Z","shell.execute_reply.started":"2021-08-02T04:26:25.649972Z","shell.execute_reply":"2021-08-02T04:26:25.655697Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class RoBERTaDataset(Dataset):\n    def __init__(self, df, tokenizer, for_test=False):\n        super().__init__()\n        self.text = df['excerpt'].values\n        self.for_test = for_test\n        if not for_test:\n            self.target = df['target'].values\n        self.max_len = Config.max_len\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(text,\n                                            None,\n                                            truncation=True,\n                                            add_special_tokens=True,\n                                            max_length=self.max_len,\n                                            padding='max_length')\n\n        if not self.for_test:\n            return {\n                'input_ids':\n                    torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask':\n                    torch.tensor(inputs['attention_mask'], dtype=torch.long),\n                'label':\n                    torch.tensor(self.target[index], dtype=torch.float)\n            }\n        else:\n            return {\n                'input_ids':\n                    torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask':\n                    torch.tensor(inputs['attention_mask'], dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.659763Z","iopub.execute_input":"2021-08-02T04:26:25.660670Z","iopub.status.idle":"2021-08-02T04:26:25.674354Z","shell.execute_reply.started":"2021-08-02T04:26:25.660623Z","shell.execute_reply":"2021-08-02T04:26:25.672970Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Attn 1","metadata":{}},{"cell_type":"code","source":"# CLRPModel: 467    BaseOne: 474  Base 2: 475     \nclass AttnOneConifg:\n    model_name = 'roberta-base'\n    pretrained_model_path = '../input/pretrained/base_s'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.676303Z","iopub.execute_input":"2021-08-02T04:26:25.677665Z","iopub.status.idle":"2021-08-02T04:26:25.689836Z","shell.execute_reply.started":"2021-08-02T04:26:25.677632Z","shell.execute_reply":"2021-08-02T04:26:25.688625Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Using Model\n# Base Model One         Base Model Two\nclass AttentionHead_Ori(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self):\n        super(CLRPModel, self).__init__()\n        config = AutoConfig.from_pretrained(AttnOneConifg.pretrained_model_path)\n        config.update({\"output_hidden_states\":True,\n                        \"hidden_dropout_prob\": 0.0,\n                        \"layer_norm_eps\": 1e-7})\n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(AttnOneConifg.pretrained_model_path, config=config)\n        self.head = AttentionHead_Ori(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.691308Z","iopub.execute_input":"2021-08-02T04:26:25.691633Z","iopub.status.idle":"2021-08-02T04:26:25.707082Z","shell.execute_reply.started":"2021-08-02T04:26:25.691604Z","shell.execute_reply":"2021-08-02T04:26:25.705881Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# 467\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = AutoConfig.from_pretrained(AttnOneConifg.pretrained_model_path)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(AttnOneConifg.pretrained_model_path, config=config)  \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n        self.regressor = nn.Sequential(nn.Linear(768, 1))\n        \n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids, attention_mask)        \n        last_hidden_states = roberta_output.hidden_states[-1]\n        \n        weights = self.attention(last_hidden_states)\n        context_vector = torch.sum(weights * last_hidden_states, dim=1)        \n        \n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.735205Z","iopub.execute_input":"2021-08-02T04:26:25.735559Z","iopub.status.idle":"2021-08-02T04:26:25.745915Z","shell.execute_reply.started":"2021-08-02T04:26:25.735533Z","shell.execute_reply":"2021-08-02T04:26:25.744364Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model MeanPooling Large and MeanEmbedding","metadata":{}},{"cell_type":"markdown","source":"MeanEmbedding完全可以和Model MeanPooling Large合在一起\n\n- MeanEmbedding V1 + SVM 473","metadata":{}},{"cell_type":"code","source":"# Mean Model\nclass MeanLargeConfig:\n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/pretrained/large_s'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512\n    \n    use_multi_sample_dropout = True # this model didn`t help","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.749142Z","iopub.execute_input":"2021-08-02T04:26:25.749716Z","iopub.status.idle":"2021-08-02T04:26:25.762050Z","shell.execute_reply.started":"2021-08-02T04:26:25.749676Z","shell.execute_reply":"2021-08-02T04:26:25.760432Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# V1 large :  inference model  同时输出 logits 和 pool_out embedding，可以选择 如何取舍两个输出。\n#             MeanEmbedding V1 + SVM 473\n#                    logits          472\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n    \nclass MeanModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(MeanLargeConfig.pretrained_model_path)\n        self.config.update({\"output_hidden_states\": True,\n                            \"hidden_dropout_prob\": 0.0,\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"layer_norm_eps\": 1e-7}) \n        self.roberta = AutoModel.from_pretrained(MeanLargeConfig.pretrained_model_path,\n                                                 config=self.config)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-7)\n        self.pooler = MeanPooling(self.config.hidden_size)\n\n        self.low_dropout = nn.Dropout(0.1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.regressor = nn.Linear(self.config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.low_dropout(pool_out)\n        \n        # didn`t help\n        logits = torch.mean(torch.stack([self.regressor(self.dropout(pool_out)) for _ in range(5)], dim=0), dim=0)\n\n        return (logits, pool_out)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.765125Z","iopub.execute_input":"2021-08-02T04:26:25.765645Z","iopub.status.idle":"2021-08-02T04:26:25.783205Z","shell.execute_reply.started":"2021-08-02T04:26:25.765577Z","shell.execute_reply":"2021-08-02T04:26:25.781914Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Attn v2","metadata":{}},{"cell_type":"code","source":"class AttConfig1:\n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/pretrained/large_s'\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    device = 'cuda'\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    head_hidden = 512\n    \n    warmup_steps = 50","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.785558Z","iopub.execute_input":"2021-08-02T04:26:25.786531Z","iopub.status.idle":"2021-08-02T04:26:25.795102Z","shell.execute_reply.started":"2021-08-02T04:26:25.786487Z","shell.execute_reply":"2021-08-02T04:26:25.793958Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, hidden_state, attention_mask):\n        att = torch.tanh(self.W(hidden_state))\n        score = self.V(att)\n\n        mask_expanded = attention_mask.unsqueeze(-1)\n        score[~mask_expanded] = -1e9\n\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * hidden_state\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass AttModel(nn.Module):\n    def __init__(self, config, attn_type='tradition'):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        self.head = AttentionHead(self.config.hidden_size, config.head_hidden)\n        \n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        self.regressor = nn.Linear(self.config.hidden_size, config.num_labels)\n        \n        self.dropout = nn.Dropout(p=0.1)\n        self.m_dropout = nn.Dropout(p=0.5)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs[2][-1]\n\n        x = self.head(hidden_states, attention_mask)\n        logits = self.regressor(x)\n\n        return logits, x","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.796745Z","iopub.execute_input":"2021-08-02T04:26:25.797468Z","iopub.status.idle":"2021-08-02T04:26:25.813504Z","shell.execute_reply.started":"2021-08-02T04:26:25.797424Z","shell.execute_reply":"2021-08-02T04:26:25.812148Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Attn 3","metadata":{}},{"cell_type":"code","source":"class AttConfig2:\n    model_name = 'roberta-base'\n    pretrained_model_path = '../input/pretrained/base_s'\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    device = 'cuda'\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    head_hidden = 512\n    \n    warmup_steps = 50","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.815215Z","iopub.execute_input":"2021-08-02T04:26:25.815742Z","iopub.status.idle":"2021-08-02T04:26:25.827223Z","shell.execute_reply.started":"2021-08-02T04:26:25.815666Z","shell.execute_reply":"2021-08-02T04:26:25.825949Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, hidden_state, attention_mask):\n\n        att = torch.tanh(self.W(hidden_state))\n        score = self.V(att)\n\n        mask_expanded = attention_mask.unsqueeze(-1)\n        score[~mask_expanded] = -1e9\n\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * hidden_state\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n    \n    \nclass AttModel2(nn.Module):\n    def __init__(self, config, attn_type='tradition'):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        self.head = AttentionHead(self.config.hidden_size, config.head_hidden)\n\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        self.regressor = nn.Sequential(\n                nn.Linear(self.config.hidden_size, 512),\n                Mish(),\n                nn.Linear(512, 1)\n        )\n        \n        self.dropout = nn.Dropout(p=0.1)\n        self.m_dropout = nn.Dropout(p=0.5)\n        \n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs[2][-1]\n\n        x = self.head(hidden_states, attention_mask)\n\n        x = self.layer_norm(x)\n        logits = self.regressor(x)\n        \n        return logits, x","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-08-02T04:26:25.828758Z","iopub.execute_input":"2021-08-02T04:26:25.829074Z","iopub.status.idle":"2021-08-02T04:26:25.846728Z","shell.execute_reply.started":"2021-08-02T04:26:25.829033Z","shell.execute_reply":"2021-08-02T04:26:25.845485Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Mean v2","metadata":{}},{"cell_type":"code","source":"# Mean Model\nclass MeanV2BaseConfig:\n    model_name = 'roberta-base'\n    pretrained_model_path = '../input/pretrained/base_s'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.848612Z","iopub.execute_input":"2021-08-02T04:26:25.849333Z","iopub.status.idle":"2021-08-02T04:26:25.860219Z","shell.execute_reply.started":"2021-08-02T04:26:25.849289Z","shell.execute_reply":"2021-08-02T04:26:25.858988Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Mean Large Model\nclass MeanV2LargeConfig:\n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/pretrained/large_s'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.864260Z","iopub.execute_input":"2021-08-02T04:26:25.864785Z","iopub.status.idle":"2021-08-02T04:26:25.871053Z","shell.execute_reply.started":"2021-08-02T04:26:25.864741Z","shell.execute_reply":"2021-08-02T04:26:25.869902Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# V2 base :  inference model\n\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        # last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        \n        return mean_embeddings\n\n\n#Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n#https://arxiv.org/abs/1908.08681v1\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n\n    \nclass MeanModel_v2(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        \n        self.pooler = MeanPooling(self.config.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        \n        self.dropout = nn.Dropout(0.5)\n        self.regressor = nn.Sequential(\n                nn.Linear(self.config.hidden_size, 512),\n                Mish(),\n                nn.Linear(512, 1)\n        )\n        \n        self.std = 0.02\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.dropout(pool_out)\n        pool_out = self.layer_norm(pool_out)\n        logits = self.regressor(pool_out)\n\n        return logits, pool_out","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.873290Z","iopub.execute_input":"2021-08-02T04:26:25.873975Z","iopub.status.idle":"2021-08-02T04:26:25.893149Z","shell.execute_reply.started":"2021-08-02T04:26:25.873931Z","shell.execute_reply":"2021-08-02T04:26:25.891993Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer func","metadata":{}},{"cell_type":"code","source":"def get_test_data(df):\n    tokenizer = torch.load('../input/pretrained/roberta_tk.pt') \n    test_dataset = RoBERTaDataset(df, tokenizer, for_test=True)\n    test_loader = DataLoader(test_dataset, batch_size=Config.test_batch,\n                             num_workers=4, shuffle=False, pin_memory=True,\n                             drop_last=False)\n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.894815Z","iopub.execute_input":"2021-08-02T04:26:25.895624Z","iopub.status.idle":"2021-08-02T04:26:25.907779Z","shell.execute_reply.started":"2021-08-02T04:26:25.895569Z","shell.execute_reply":"2021-08-02T04:26:25.906528Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def reset_memory():\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.909076Z","iopub.execute_input":"2021-08-02T04:26:25.910905Z","iopub.status.idle":"2021-08-02T04:26:25.920257Z","shell.execute_reply.started":"2021-08-02T04:26:25.910833Z","shell.execute_reply":"2021-08-02T04:26:25.918749Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def inference(test_dataloader, model_dirs, model=None, n_models=5, ckpt_bias=0, with_embedding=False):\n    models_preds = []\n    models_embedding = []\n    for model_num in range(n_models):\n        print(f'{by_}{r_}  >>> Inference # {model_num+1}/{n_models}  {sr_}')\n        torch.cuda.synchronize()\n\n        # load\n        model_path = model_dirs[model_num]\n        print(f\" ### Using {model_path}\")\n        if model:\n            model.load_state_dict(torch.load(model_path, map_location=Config.device))\n        else:\n            model = torch.load(model_path)\n        model.to(Config.device)\n\n        # predict\n        fold_preds = []\n        embeddings = []\n        model.eval()\n        with torch.no_grad():\n            for step, batch in enumerate(test_dataloader):\n                sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n                preds = model(sent_id, mask)\n\n                if with_embedding and len(preds) == 2:\n                    preds, embed = preds[0], preds[1]\n                    embed = embed.detach().cpu().numpy()\n                    embeddings.extend(embed)\n                if len(preds) == 2:\n                    preds = preds[0]\n                fold_preds += preds.flatten().cpu().tolist()\n\n        # records\n        models_preds.append(fold_preds)\n        if with_embedding:\n            models_embedding.append(np.array(embeddings))\n\n        if not model:  # load_state_dict 方式，不能在这里删除\n            del model\n            gc.collect()\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n\n        print(f'! Model Complete. ++++++++++')\n    print()\n\n    # output\n    models_preds = np.array(models_preds).mean(axis=0)\n    if not with_embedding:\n        return models_preds\n    else:\n        return models_preds, models_embedding","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.922587Z","iopub.execute_input":"2021-08-02T04:26:25.923421Z","iopub.status.idle":"2021-08-02T04:26:25.940763Z","shell.execute_reply.started":"2021-08-02T04:26:25.923376Z","shell.execute_reply":"2021-08-02T04:26:25.939591Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def embedding_svr_test(embeddings, num_pred, bert_nums=5, svr_nfolds=10):\n    # SVM predict: 5 SVR model\n    results = np.zeros(num_pred)\n    for index, X_test in enumerate(embeddings):\n        print(f'{by_}{r_}  SVR#{index+1} predicting {sr_}')\n        for i in range(svr_nfolds):\n            svr = load(save_dir + f'svr_{index}_{i}.bin')\n            preds = svr.predict(X_test)\n            results += preds\n    print(f'SVR Complete.')\n\n    return results / bert_nums / svr_nfolds","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.944556Z","iopub.execute_input":"2021-08-02T04:26:25.944939Z","iopub.status.idle":"2021-08-02T04:26:25.953861Z","shell.execute_reply.started":"2021-08-02T04:26:25.944910Z","shell.execute_reply":"2021-08-02T04:26:25.952777Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## MAIN","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda x: x.replace('\\n',' '))\n\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:25.956762Z","iopub.execute_input":"2021-08-02T04:26:25.957816Z","iopub.status.idle":"2021-08-02T04:26:26.120818Z","shell.execute_reply.started":"2021-08-02T04:26:25.957712Z","shell.execute_reply":"2021-08-02T04:26:26.119800Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def run_m1():\n    #### LitModel: 467\n    test_dataloader = get_test_data(test_df)\n    \n    Lit = [f'../input/comlitothers/model_{i + 1}.bin' for i in range(5)]\n    litmodel = LitModel()  # if use load_state_dict, init model\n    pred_lit = inference(test_dataloader, Lit, model=litmodel, ckpt_bias=1)\n    del litmodel, test_dataloader\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()\n    return pred_lit","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:26.122688Z","iopub.execute_input":"2021-08-02T04:26:26.123148Z","iopub.status.idle":"2021-08-02T04:26:26.130241Z","shell.execute_reply.started":"2021-08-02T04:26:26.123104Z","shell.execute_reply":"2021-08-02T04:26:26.129051Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def run_m2():\n    #### mean large v2 reinit: 0.466\n    test_dataloader = get_test_data(test_df)\n    \n    MeanL_4 = [ f'/kaggle/input/meanlargereinit/model_{i}.bin' for i in range(5)]\n    config = MeanV2LargeConfig()\n    meanmodel4 = MeanModel_v2(config)\n    pred_mean_v4 = inference(test_dataloader, MeanL_4, model=meanmodel4, with_embedding=False)\n    # svr_preds = embedding_svr_test(embeddings, len(test_df))\n    del meanmodel4, test_dataloader\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()\n\n    return pred_mean_v4","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:26.131994Z","iopub.execute_input":"2021-08-02T04:26:26.132632Z","iopub.status.idle":"2021-08-02T04:26:26.141073Z","shell.execute_reply.started":"2021-08-02T04:26:26.132587Z","shell.execute_reply":"2021-08-02T04:26:26.140066Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def run_m3():\n    #### attn large 1:  0.464\n    test_dataloader = get_test_data(test_df)\n    \n    AttL_1 = [ f'/kaggle/input/largeattnlit/model_{i}.bin' for i in range(5)]\n    config = AttConfig1()\n    attmodel1 = AttModel(config)\n    pred_attL_v1 = inference(test_dataloader, AttL_1, model=attmodel1, with_embedding=False)\n    # svr_preds = embedding_svr_test(embeddings, len(test_df))\n    del attmodel1, test_dataloader\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()\n    return pred_attL_v1","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:26.142696Z","iopub.execute_input":"2021-08-02T04:26:26.143264Z","iopub.status.idle":"2021-08-02T04:26:26.152486Z","shell.execute_reply.started":"2021-08-02T04:26:26.143219Z","shell.execute_reply":"2021-08-02T04:26:26.150711Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def run_m4():\n    #### attn large reinit 2:  0.467\n    test_dataloader = get_test_data(test_df)\n    \n    AttL_2 = [ f'/kaggle/input/attlargereinit/model_{i}.bin' for i in range(5)]\n    config = AttConfig1()\n    attmodel2 = AttModel(config)\n    pred_attL_v2 = inference(test_dataloader, AttL_2, model=attmodel2, with_embedding=False)\n    # svr_preds = embedding_svr_test(embeddings, len(test_df))\n    del attmodel2, test_dataloader\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()\n    return pred_attL_v2","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:26.154822Z","iopub.execute_input":"2021-08-02T04:26:26.155287Z","iopub.status.idle":"2021-08-02T04:26:26.166203Z","shell.execute_reply.started":"2021-08-02T04:26:26.155244Z","shell.execute_reply":"2021-08-02T04:26:26.164942Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"################### Post Process ########################\npred_attL_v2 = run_m4()\npred_attL_v1 = run_m3()\npred_mean_v4 = run_m2()\npred_lit = run_m1()\n\n\n# predictions = pred_attL_v1 * 0.5 + pred_mean_v4 * 0.3 + pred_lit * 0.2\npredictions = pred_attL_v2 * 0.15 + pred_attL_v1 * 0.4 + pred_mean_v4 * 0.3 + pred_lit * 0.15\n\n# predictions = svr_preds * 0.5 + pred_lit * 0.5  # or whatever\n\n\n################### Submisson ########################\nsubmission_df.target = predictions\nsubmission_df.to_csv('submission.csv')\nprint(submission_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:26:26.167474Z","iopub.execute_input":"2021-08-02T04:26:26.167805Z","iopub.status.idle":"2021-08-02T04:29:42.519819Z","shell.execute_reply.started":"2021-08-02T04:26:26.167761Z","shell.execute_reply":"2021-08-02T04:29:42.518813Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/pretrained/large_s were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at ../input/pretrained/large_s and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[43m\u001b[31m  >>> Inference # 1/5  \u001b[0m\n ### Using /kaggle/input/largeattnlit/model_0.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 2/5  \u001b[0m\n ### Using /kaggle/input/largeattnlit/model_1.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 3/5  \u001b[0m\n ### Using /kaggle/input/largeattnlit/model_2.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 4/5  \u001b[0m\n ### Using /kaggle/input/largeattnlit/model_3.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 5/5  \u001b[0m\n ### Using /kaggle/input/largeattnlit/model_4.bin\n! Model Complete. ++++++++++\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/pretrained/large_s were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at ../input/pretrained/large_s and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[43m\u001b[31m  >>> Inference # 1/5  \u001b[0m\n ### Using /kaggle/input/meanlargereinit/model_0.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 2/5  \u001b[0m\n ### Using /kaggle/input/meanlargereinit/model_1.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 3/5  \u001b[0m\n ### Using /kaggle/input/meanlargereinit/model_2.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 4/5  \u001b[0m\n ### Using /kaggle/input/meanlargereinit/model_3.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 5/5  \u001b[0m\n ### Using /kaggle/input/meanlargereinit/model_4.bin\n! Model Complete. ++++++++++\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at ../input/pretrained/base_s were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaModel were not initialized from the model checkpoint at ../input/pretrained/base_s and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[43m\u001b[31m  >>> Inference # 1/5  \u001b[0m\n ### Using ../input/comlitothers/model_1.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 2/5  \u001b[0m\n ### Using ../input/comlitothers/model_2.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 3/5  \u001b[0m\n ### Using ../input/comlitothers/model_3.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 4/5  \u001b[0m\n ### Using ../input/comlitothers/model_4.bin\n! Model Complete. ++++++++++\n\u001b[43m\u001b[31m  >>> Inference # 5/5  \u001b[0m\n ### Using ../input/comlitothers/model_5.bin\n! Model Complete. ++++++++++\n\n          id    target\n0  c0f722661 -0.406300\n1  f0953f0a5 -0.482320\n2  0df072751 -0.514308\n3  04caf4e0c -2.333567\n4  0e63f8bea -1.933935\n5  12537fe78 -1.253635\n6  965e592c0  0.236844\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n#     test_df['excerpt'] = test_df['excerpt'].apply(lambda x: x.replace('\\n',' '))\n#     test_dataloader = get_test_data(test_df)\n\n\n#     ################### Predict ########################\n#     ################## load model\n#     #### Base CLRPModel 474\n# #     CLRP_1 = [f'../input/comlitmodelone/best_model_{i}.pt' for i in range(5)]\n# #     preds_c1 = inference(test_dataloader, CLRP_1, with_embedding=False)\n# #     reset_memory()\n\n\n#     #### Base CLRPModel 475\n# #     CLRP_2 = [f'../input/comlitbase2/best_model_{i}.pt' for i in range(5)]\n# #     preds_c2 = inference(test_dataloader, CLRP_2, with_embedding=False)\n# #     reset_memory()\n\n\n\n#     ################## load_state_dict\n#     #### LitModel: 467\n#     Lit = [f'../input/comlitothers/model_{i + 1}.bin' for i in range(5)]\n#     litmodel = LitModel()  # if use load_state_dict, init model\n#     pred_lit = inference(test_dataloader, Lit, model=litmodel, ckpt_bias=1)\n#     del litmodel\n#     reset_memory()\n\n\n#     #### mean large v1: 472  ;    MeanEmbedding V1 + SVM 473\n# #     MeanL_1 = [ f'../input/clrobertalarger/model_{i}.bin' for i in range(5)]\n# #     meanmodel = MeanModel()\n# #     pred_mean_v1, embeddings = inference(test_dataloader, MeanL_1, model=meanmodel, with_embedding=True)\n# #     svr_preds = embedding_svr_test(embeddings, len(test_df))\n# #     del meanmodel\n# #     reset_memory()\n\n\n#     #### mean base v2: 478 bad\n# #     MeanB_2 = [ f'../input/newmeanbase/model_{i}.bin' for i in range(5)]\n# #     config = MeanV2BaseConfig()\n# #     meanmodel2 = MeanModel_v2(config)\n# #     pred_mean_v2 = inference(test_dataloader, MeanB_2, model=meanmodel2, with_embedding=False)\n# #     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n# #     del meanmodel2\n# #     reset_memory()\n\n\n#     #### mean base fgm v2: 481 bad     fgm not help\n# #     MeanBF_2 = [ f'../input/newmeanbasefgm/model_{i}.bin' for i in range(5)]\n# #     config = MeanV2BaseConfig()\n# #     meanmodel2f = MeanModel_v2(config)\n# #     pred_mean_v2f = inference(test_dataloader, MeanBF_2, model=meanmodel2f, with_embedding=False)\n# #     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n# #     del meanmodel2f\n# #     reset_memory()\n    \n    \n#     #### mean large v2: 0.469  Nice\n# #     MeanL_3 = [ f'/kaggle/input/newmeanlarge/model_{i}.bin' for i in range(5)]\n# #     config = MeanV2LargeConfig()\n# #     meanmodel3 = MeanModel_v2(config)\n# #     pred_mean_v3 = inference(test_dataloader, MeanL_3, model=meanmodel3, with_embedding=False)\n# #     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n# #     del meanmodel3\n# #     reset_memory()\n\n\n#     #### mean large v2 reinit: 0.466\n#     MeanL_4 = [ f'/kaggle/input/meanlargereinit/model_{i}.bin' for i in range(5)]\n#     config = MeanV2LargeConfig()\n#     meanmodel4 = MeanModel_v2(config)\n#     pred_mean_v4 = inference(test_dataloader, MeanL_4, model=meanmodel4, with_embedding=False)\n#     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n#     del meanmodel4\n#     reset_memory()\n    \n\n#     #### attn large 1:  0.464\n#     AttL_1 = [ f'/kaggle/input/largeattnlit/model_{i}.bin' for i in range(5)]\n#     config = AttConfig1()\n#     attmodel1 = AttModel(config)\n#     pred_attL_v1 = inference(test_dataloader, AttL_1, model=attmodel1, with_embedding=False)\n#     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n#     del attmodel1\n#     reset_memory()\n\n\n#     #### attn large reinit 2:  0.467\n#     AttL_2 = [ f'/kaggle/input/attlargereinit/model_{i}.bin' for i in range(5)]\n#     config = AttConfig1()\n#     attmodel2 = AttModel(config)\n#     pred_attL_v2 = inference(test_dataloader, AttL_2, model=attmodel2, with_embedding=False)\n#     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n#     del attmodel2\n#     reset_memory()\n    \n    \n#     #### attn base 2: \n# #     AttB_2 = [ f'/kaggle/input/meanattnreinit/model_{i}.bin' for i in range(5)]\n# #     config = AttConfig2()\n# #     attmodel2 = AttModel2(config)\n# #     pred_attL_v2 = inference(test_dataloader, AttL_2, model=attmodel2, with_embedding=False)\n# #     # svr_preds = embedding_svr_test(embeddings, len(test_df))\n# #     del attmodel2\n# #     reset_memory()\n    \n#     # ................................................\n\n    \n#     ################### Post Process ########################\n#     predictions = pred_attL_v2 * 0.15 + pred_attL_v1 * 0.4 + pred_mean_v4 * 0.3 + pred_lit * 0.15\n# #     predictions = svr_preds * 0.5 + pred_lit * 0.5  # or whatever\n\n\n#     ################### Submisson ########################\n#     result_df = pd.DataFrame({'id': test_df.id, 'target': predictions})\n#     result_df.to_csv('submission.csv', index=False)\n#     print(result_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T04:29:42.521506Z","iopub.execute_input":"2021-08-02T04:29:42.521931Z","iopub.status.idle":"2021-08-02T04:29:42.529147Z","shell.execute_reply.started":"2021-08-02T04:29:42.521891Z","shell.execute_reply":"2021-08-02T04:29:42.527529Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#### Only SVR inference[Test using]","metadata":{}},{"cell_type":"markdown","source":"-   id    target\n- 0  c0f722661 -0.367501\n- 1  f0953f0a5 -0.398820\n- 2  0df072751 -0.537474\n- 3  04caf4e0c -2.299636\n- 4  0e63f8bea -1.962926\n- 5  12537fe78 -1.093182\n- 6  965e592c0  0.077126","metadata":{}}]}