{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-02T06:59:25.18198Z","iopub.execute_input":"2021-08-02T06:59:25.182359Z","iopub.status.idle":"2021-08-02T06:59:25.290371Z","shell.execute_reply.started":"2021-08-02T06:59:25.182277Z","shell.execute_reply":"2021-08-02T06:59:25.289515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom joblib import dump, load\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom time import time\nfrom colorama import Fore, Back, Style\n\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nby_ = Back.YELLOW\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:25.293533Z","iopub.execute_input":"2021-08-02T06:59:25.293848Z","iopub.status.idle":"2021-08-02T06:59:32.693963Z","shell.execute_reply.started":"2021-08-02T06:59:25.293819Z","shell.execute_reply":"2021-08-02T06:59:32.693084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global config","metadata":{}},{"cell_type":"code","source":"class Config:\n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    num_labels = 1","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.69562Z","iopub.execute_input":"2021-08-02T06:59:32.695945Z","iopub.status.idle":"2021-08-02T06:59:32.703963Z","shell.execute_reply.started":"2021-08-02T06:59:32.695905Z","shell.execute_reply":"2021-08-02T06:59:32.703023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class RoBERTaDataset(Dataset):\n    def __init__(self, df, tokenizer, for_test=False):\n        super().__init__()\n        self.text = df['excerpt'].values\n        self.for_test = for_test\n        if not for_test:\n            self.target = df['target'].values\n        self.max_len = Config.max_len\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(text,\n                                            None,\n                                            truncation=True,\n                                            add_special_tokens=True,\n                                            max_length=self.max_len,\n                                            padding='max_length')\n\n        if not self.for_test:\n            return {\n                'input_ids':\n                    torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask':\n                    torch.tensor(inputs['attention_mask'], dtype=torch.long),\n                'label':\n                    torch.tensor(self.target[index], dtype=torch.float)\n            }\n        else:\n            return {\n                'input_ids':\n                    torch.tensor(inputs['input_ids'], dtype=torch.long),\n                'attention_mask':\n                    torch.tensor(inputs['attention_mask'], dtype=torch.long)\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.705986Z","iopub.execute_input":"2021-08-02T06:59:32.706348Z","iopub.status.idle":"2021-08-02T06:59:32.719183Z","shell.execute_reply.started":"2021-08-02T06:59:32.706308Z","shell.execute_reply":"2021-08-02T06:59:32.718166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Attn 1","metadata":{}},{"cell_type":"code","source":"# CLRPModel: 467    BaseOne: 474  Base 2: 475     \nclass AttnOneConifg:\n    model_name = 'roberta-base'\n    pretrained_model_path = '/kaggle/input/comlitrobertabasescript/'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.720793Z","iopub.execute_input":"2021-08-02T06:59:32.721065Z","iopub.status.idle":"2021-08-02T06:59:32.729686Z","shell.execute_reply.started":"2021-08-02T06:59:32.721024Z","shell.execute_reply":"2021-08-02T06:59:32.728762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Using Model\n# Base Model One         Base Model Two\nclass AttentionHead_Ori(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self):\n        super(CLRPModel, self).__init__()\n        config = AutoConfig.from_pretrained(AttnOneConifg.pretrained_model_path)\n        config.update({\"output_hidden_states\":True,\n                        \"hidden_dropout_prob\": 0.0,\n                        \"layer_norm_eps\": 1e-7})\n        self.h_size = config.hidden_size\n        self.transformer = AutoModel.from_pretrained(AttnOneConifg.pretrained_model_path, config=config)\n        self.head = AttentionHead_Ori(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        context = self.head(transformer_out.last_hidden_state)\n        x = self.linear(context)\n        return x, context","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.731759Z","iopub.execute_input":"2021-08-02T06:59:32.732054Z","iopub.status.idle":"2021-08-02T06:59:32.746534Z","shell.execute_reply.started":"2021-08-02T06:59:32.732027Z","shell.execute_reply":"2021-08-02T06:59:32.745645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = AutoConfig.from_pretrained(AttnOneConifg.pretrained_model_path)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(AttnOneConifg.pretrained_model_path, config=config)  \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n        self.regressor = nn.Sequential(nn.Linear(768, 1))\n        \n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids, attention_mask)        \n        last_hidden_states = roberta_output.hidden_states[-1]\n        \n        weights = self.attention(last_hidden_states)\n        context_vector = torch.sum(weights * last_hidden_states, dim=1)        \n        \n        return self.regressor(context_vector), context_vector","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.75002Z","iopub.execute_input":"2021-08-02T06:59:32.750334Z","iopub.status.idle":"2021-08-02T06:59:32.761315Z","shell.execute_reply.started":"2021-08-02T06:59:32.750305Z","shell.execute_reply":"2021-08-02T06:59:32.760293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model MeanPooling Large and MeanEmbedding","metadata":{}},{"cell_type":"markdown","source":"MeanEmbedding完全可以和Model MeanPooling Large合在一起\n\n- MeanEmbedding V1 + SVM 473","metadata":{}},{"cell_type":"code","source":"# Mean Model\nclass MeanLargeConfig:\n    model_name = 'roberta-large'\n    pretrained_model_path = '../input/comlitrobertalargescript'\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512\n    \n    use_multi_sample_dropout = True # this model didn`t help","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.76444Z","iopub.execute_input":"2021-08-02T06:59:32.765849Z","iopub.status.idle":"2021-08-02T06:59:32.773424Z","shell.execute_reply.started":"2021-08-02T06:59:32.765809Z","shell.execute_reply":"2021-08-02T06:59:32.77263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# V1 large :  inference model  同时输出 logits 和 pool_out embedding，可以选择 如何取舍两个输出。\n#             MeanEmbedding V1 + SVM 473\n#                    logits          472\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n    \nclass MeanModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(MeanLargeConfig.pretrained_model_path)\n        self.config.update({\"output_hidden_states\": True,\n                            \"hidden_dropout_prob\": 0.0,\n                            \"attention_probs_dropout_prob\": 0.1,\n                            \"layer_norm_eps\": 1e-7}) \n        self.roberta = AutoModel.from_pretrained(MeanLargeConfig.pretrained_model_path,\n                                                 config=self.config)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-7)\n        self.pooler = MeanPooling(self.config.hidden_size)\n\n        self.low_dropout = nn.Dropout(0.1)\n        self.dropout = nn.Dropout(p=0.5)\n        self.regressor = nn.Linear(self.config.hidden_size, 1)\n\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.low_dropout(pool_out)\n        \n        # didn`t help\n        logits = torch.mean(torch.stack([self.regressor(self.dropout(pool_out)) for _ in range(5)], dim=0), dim=0)\n\n        return (logits, pool_out)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.777283Z","iopub.execute_input":"2021-08-02T06:59:32.777618Z","iopub.status.idle":"2021-08-02T06:59:32.791076Z","shell.execute_reply.started":"2021-08-02T06:59:32.77759Z","shell.execute_reply":"2021-08-02T06:59:32.79025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attn v2","metadata":{}},{"cell_type":"code","source":"class AttConfig1:\n    model_name = 'roberta-large'\n    pretrained_model_path = ''\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    device = 'cuda'\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    head_hidden = 512\n    \n    warmup_steps = 50","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.794126Z","iopub.execute_input":"2021-08-02T06:59:32.794344Z","iopub.status.idle":"2021-08-02T06:59:32.801428Z","shell.execute_reply.started":"2021-08-02T06:59:32.794324Z","shell.execute_reply":"2021-08-02T06:59:32.800649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, hidden_state, attention_mask):\n        att = torch.tanh(self.W(hidden_state))\n        score = self.V(att)\n\n        mask_expanded = attention_mask.unsqueeze(-1)\n        score[~mask_expanded] = -1e9\n\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * hidden_state\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass AttModel(nn.Module):\n    def __init__(self, config, attn_type='tradition'):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        self.head = AttentionHead(self.config.hidden_size, config.head_hidden)\n        \n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        self.regressor = nn.Linear(self.config.hidden_size, config.num_labels)\n        \n        self.dropout = nn.Dropout(p=0.1)\n        self.m_dropout = nn.Dropout(p=0.5)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs[2][-1]\n\n        x = self.head(hidden_states, attention_mask)\n        logits = self.regressor(x)\n\n        return logits, x","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.802916Z","iopub.execute_input":"2021-08-02T06:59:32.803334Z","iopub.status.idle":"2021-08-02T06:59:32.818802Z","shell.execute_reply.started":"2021-08-02T06:59:32.803296Z","shell.execute_reply":"2021-08-02T06:59:32.817728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attn 3","metadata":{}},{"cell_type":"code","source":"class AttConfig2:\n    model_name = 'roberta-base'\n    pretrained_model_path = ''\n    \n    output_hidden_states = True\n    epochs = 3\n    num_labels = 1\n    \n    device = 'cuda'\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    head_hidden = 512\n    \n    warmup_steps = 50","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.820466Z","iopub.execute_input":"2021-08-02T06:59:32.821019Z","iopub.status.idle":"2021-08-02T06:59:32.828019Z","shell.execute_reply.started":"2021-08-02T06:59:32.820978Z","shell.execute_reply":"2021-08-02T06:59:32.827074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, hidden_state, attention_mask):\n\n        att = torch.tanh(self.W(hidden_state))\n        score = self.V(att)\n\n        mask_expanded = attention_mask.unsqueeze(-1)\n        score[~mask_expanded] = -1e9\n\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * hidden_state\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n    \n    \nclass AttModel2(nn.Module):\n    def __init__(self, config, attn_type='tradition'):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        self.head = AttentionHead(self.config.hidden_size, config.head_hidden)\n\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        self.regressor = nn.Sequential(\n                nn.Linear(self.config.hidden_size, 512),\n                Mish(),\n                nn.Linear(512, 1)\n        )\n        \n        self.dropout = nn.Dropout(p=0.1)\n        self.m_dropout = nn.Dropout(p=0.5)\n        \n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs[2][-1]\n\n        x = self.head(hidden_states, attention_mask)\n\n        x = self.layer_norm(x)\n        logits = self.regressor(x)\n        \n        return logits, x","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.829726Z","iopub.execute_input":"2021-08-02T06:59:32.830325Z","iopub.status.idle":"2021-08-02T06:59:32.846823Z","shell.execute_reply.started":"2021-08-02T06:59:32.830287Z","shell.execute_reply":"2021-08-02T06:59:32.845834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean v2","metadata":{}},{"cell_type":"code","source":"# Mean Model\nclass MeanV2BaseConfig:\n    model_name = 'roberta-base'\n    pretrained_model_path = ''\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.848361Z","iopub.execute_input":"2021-08-02T06:59:32.848922Z","iopub.status.idle":"2021-08-02T06:59:32.857484Z","shell.execute_reply.started":"2021-08-02T06:59:32.848877Z","shell.execute_reply":"2021-08-02T06:59:32.856516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean Large Model\nclass MeanV2LargeConfig:\n    model_name = 'roberta-large'\n    pretrained_model_path = ''\n    \n    epochs = 3\n    batch_size = 16\n    test_batch = 32\n    \n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    weight_decay = 0.01\n    \n    head_hidden = 512","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.861176Z","iopub.execute_input":"2021-08-02T06:59:32.861467Z","iopub.status.idle":"2021-08-02T06:59:32.869038Z","shell.execute_reply.started":"2021-08-02T06:59:32.861434Z","shell.execute_reply":"2021-08-02T06:59:32.868119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# V2 base :  inference model\n\nclass MeanPooling(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        \n    def forward(self, hidden_state, attention_mask):\n        # last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_state.size()).float()\n        sum_embeddings = torch.sum(hidden_state * input_mask_expanded, 1)\n\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        \n        return mean_embeddings\n\n\n#Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n#https://arxiv.org/abs/1908.08681v1\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n\n    \nclass MeanModel_v2(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(config.pretrained_model_path)\n        self.config.update({\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.0,\n                \"attention_probs_dropout_prob\": 0.1,\n                \"layer_norm_eps\": 1e-7\n                }) \n        self.roberta = AutoModel.from_pretrained(config.pretrained_model_path,\n                                                 config=self.config)\n        \n        self.pooler = MeanPooling(self.config.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-5)\n        \n        self.dropout = nn.Dropout(0.5)\n        self.regressor = nn.Sequential(\n                nn.Linear(self.config.hidden_size, 512),\n                Mish(),\n                nn.Linear(512, 1)\n        )\n        \n        self.std = 0.02\n        self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            # module.weight.data.normal_(mean=0.0, std=self.std)\n            init.kaiming_normal_(module.weight, mode='fan_in')\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n        \n        ## MeanPooling\n        hidden_states = outputs[0]\n        pool_out = self.pooler(hidden_states, attention_mask)\n        # pool_out = self.dropout(pool_out)\n        pool_out = self.layer_norm(pool_out)\n        logits = self.regressor(pool_out)\n\n        return logits, pool_out","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.872224Z","iopub.execute_input":"2021-08-02T06:59:32.872504Z","iopub.status.idle":"2021-08-02T06:59:32.891838Z","shell.execute_reply.started":"2021-08-02T06:59:32.872463Z","shell.execute_reply":"2021-08-02T06:59:32.890838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Infer func","metadata":{}},{"cell_type":"code","source":"def get_test_data(df):\n    tokenizer = torch.load('/kaggle/input/tokenizer/roberta_tk.pt') \n    test_dataset = RoBERTaDataset(df, tokenizer, for_test=True)\n    test_loader = DataLoader(test_dataset, batch_size=32,\n                             num_workers=4, shuffle=False, pin_memory=True,\n                             drop_last=False)\n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.893297Z","iopub.execute_input":"2021-08-02T06:59:32.893924Z","iopub.status.idle":"2021-08-02T06:59:32.90209Z","shell.execute_reply.started":"2021-08-02T06:59:32.89386Z","shell.execute_reply":"2021-08-02T06:59:32.90121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_memory():\n    gc.collect()\n    torch.cuda.synchronize()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.903566Z","iopub.execute_input":"2021-08-02T06:59:32.904015Z","iopub.status.idle":"2021-08-02T06:59:32.911239Z","shell.execute_reply.started":"2021-08-02T06:59:32.90394Z","shell.execute_reply":"2021-08-02T06:59:32.910385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(test_dataloader, model_dirs, model=None, n_models=5, ckpt_bias=0, with_embedding=False):\n    models_preds = []\n    models_embedding = []\n    for model_num in range(n_models):\n        print(f'{by_}{r_}  >>> Inference # {model_num+1}/{n_models}  {sr_}')\n        torch.cuda.synchronize()\n\n        # load\n        model_path = model_dirs[model_num]\n        print(f\" ### Using {model_path}\")\n        if model:\n            model.load_state_dict(torch.load(model_path, map_location=Config.device))\n        else:\n            model = torch.load(model_path)\n        model.to(Config.device)\n\n        # predict\n        fold_preds = []\n        embeddings = []\n        model.eval()\n        with torch.no_grad():\n            for step, batch in enumerate(test_dataloader):\n                sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n                preds = model(sent_id, mask)\n                \n                if with_embedding and len(preds) == 2:\n                    preds, embed = preds[0], preds[1]\n                    embed = embed.detach().cpu().numpy()\n                    embeddings.extend(embed)\n                if len(preds) == 2:\n                    preds = preds[0]\n                fold_preds += preds.flatten().cpu().tolist()\n\n        # records\n        models_preds.append(fold_preds)\n        if with_embedding:\n            models_embedding.append(np.array(embeddings))\n\n        if not model:  # load_state_dict 方式，不能在这里删除\n            del model\n            gc.collect()\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n\n        print(f'! Model Complete. ++++++++++')\n    print()\n\n    # output\n    models_preds = np.array(models_preds).mean(axis=0)\n    if not with_embedding:\n        return models_preds\n    else:\n        return models_preds, models_embedding","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.913083Z","iopub.execute_input":"2021-08-02T06:59:32.91339Z","iopub.status.idle":"2021-08-02T06:59:32.926831Z","shell.execute_reply.started":"2021-08-02T06:59:32.913364Z","shell.execute_reply":"2021-08-02T06:59:32.925795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def embedding_svr_test(embeddings, num_pred, bert_nums=5, svr_nfolds=10):\n    # SVM predict: 5 SVR model\n    results = np.zeros(num_pred)\n    for index, X_test in enumerate(embeddings):\n        print(f'{by_}{r_}  SVR#{index+1} predicting {sr_}')\n        for i in range(svr_nfolds):\n            svr = load(save_dir + f'svr_{index}_{i}.bin')\n            preds = svr.predict(X_test)\n            results += preds\n    print(f'SVR Complete.')\n\n    return results / bert_nums / svr_nfolds","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.928674Z","iopub.execute_input":"2021-08-02T06:59:32.929124Z","iopub.status.idle":"2021-08-02T06:59:32.937841Z","shell.execute_reply.started":"2021-08-02T06:59:32.929084Z","shell.execute_reply":"2021-08-02T06:59:32.936886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install textstat --no-index --find-links=file:///kaggle/input/textstat-local/textstat \n\n# import textstat\n\n# def add_textstat_features(df):\n#     ### You can add/remove any feature below and it will be used in training and test\n#     df['coleman_liau_index'] = df['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\n#     df['flesch_reading_ease'] = df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n#     df['smog_index'] = df['excerpt'].apply(lambda x: textstat.smog_index(x))\n#     df['dale_chall_readability_score'] = df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n#     return df\n\n# def difficult_words_ratio(df, input_col='excerpt', output_col='difficult_words_ratio'):\n#     print(f\"Applying {output_col} to data set.\")\n#     df[output_col] = df[input_col].apply(lambda x: textstat.difficult_words(x))\n#     df[output_col] = df.apply(lambda x: x[output_col] / textstat.lexicon_count(x[input_col]), axis=1)\n#     return df \n\n# def syllable_ratio(df, input_col='excerpt', output_col='syllable_ratio'):\n#     print(f\"Applying {output_col} to data set.\")\n#     df[output_col] = df[input_col].apply(lambda x: textstat.syllable_count(x))\n#     df[output_col] = df.apply(lambda x: x[output_col] / textstat.lexicon_count(x[input_col]), axis=1)\n#     return df","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.93939Z","iopub.execute_input":"2021-08-02T06:59:32.940077Z","iopub.status.idle":"2021-08-02T06:59:32.947566Z","shell.execute_reply.started":"2021-08-02T06:59:32.940037Z","shell.execute_reply":"2021-08-02T06:59:32.94672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_embeddings(embeds, path):\n    import pickle\n    with open(path, \"wb\") as f:\n        pickle.dump(embeds, f)\n\ndef load_embeddings(path):\n    import pickle\n    with open(path, \"rb\") as f:\n        emb = pickle.load(f)\n    return emb","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.94919Z","iopub.execute_input":"2021-08-02T06:59:32.949475Z","iopub.status.idle":"2021-08-02T06:59:32.956634Z","shell.execute_reply.started":"2021-08-02T06:59:32.949448Z","shell.execute_reply":"2021-08-02T06:59:32.95558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train SVR","metadata":{}},{"cell_type":"code","source":"# test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/cmlit-fold/train_data.csv\")\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda x: x.replace('\\n',' '))\n\n\n\ntest_dataloader = get_test_data(test_df)\n#### LitModel:\nLit = [f'../input/comlitothers/model_{i + 1}.bin' for i in range(5)]\nlitmodel = LitModel()  # if use load_state_dict, init model\npred_litm, embed1 = inference(test_dataloader, Lit, model=litmodel, ckpt_bias=1,with_embedding=True)\ntest_df[\"m1\"] = pred_litm\nsave_embeddings(embed1, \"./embed1.bin\")\n\ndel litmodel, test_dataloader\nreset_memory()\n\n\n# #### mean large v2:\n# MeanL_3 = [ f'/kaggle/input/newmeanlarge/model_{i}.bin' for i in range(5)]\n# config = MeanV2LargeConfig()\n# meanmodel3 = MeanModel_v2(config)\n# pred_mean_v3, embed2 = inference(test_dataloader, MeanL_3, model=meanmodel3, with_embedding=True)\n# # svr_preds = embedding_svr_test(embeddings, len(test_df))\n# test_df[\"m2\"] = pred_mean_v3\n# save_embeddings(embed2, \"./embed2.bin\")\n\n# del meanmodel3\n# reset_memory()\n\n\ntest_dataloader = get_test_data(test_df)\n#### attn large 1:\nAttL_1 = [ f'/kaggle/input/largeattnlit/model_{i}.bin' for i in range(5)]\nconfig = AttConfig1()\nattmodel1 = AttModel(config)\npred_attL_v1, embed3 = inference(test_dataloader, AttL_1, model=attmodel1, with_embedding=True)\n# svr_preds = embedding_svr_test(embeddings, len(test_df))\ntest_df[\"m3\"] = pred_attL_v1\nsave_embeddings(embed3, \"./embed3.bin\")\n\ndel attmodel1, test_dataloader\nreset_memory()\n\n\n\ntest_dataloader = get_test_data(test_df)\n#### mean large v2 reinit:\nMeanL_4 = [ f'/kaggle/input/meanlargereinit/model_{i}.bin' for i in range(5)]\nconfig = MeanV2LargeConfig()\nmeanmodel4 = MeanModel_v2(config)\npred_mean_v4, embed4 = inference(test_dataloader, MeanL_4, model=meanmodel4, with_embedding=True)\n# svr_preds = embedding_svr_test(embeddings, len(test_df))\ntest_df[\"m4\"] = pred_mean_v4\nsave_embeddings(embed4, \"./embed4.bin\")\n\ndel meanmodel4, test_dataloader\nreset_memory()\n\n\n\ntest_dataloader = get_test_data(test_df)\n#### attn large reinit 2:\nAttL_2 = [ f'/kaggle/input/attlargereinit/model_{i}.bin' for i in range(5)]\nconfig = AttConfig1()\nattmodel2 = AttModel(config)\npred_attL_v2, embed2  = inference(test_dataloader, AttL_2, model=attmodel2, with_embedding=True)\n# svr_preds = embedding_svr_test(embeddings, len(test_df))\ntest_df[\"m2\"] = pred_attL_v2\nsave_embeddings(embed2, \"./embed2.bin\")\n\ndel attmodel2, test_dataloader\nreset_memory()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T06:59:32.960575Z","iopub.execute_input":"2021-08-02T06:59:32.960853Z","iopub.status.idle":"2021-08-02T07:25:49.841192Z","shell.execute_reply.started":"2021-08-02T06:59:32.960827Z","shell.execute_reply":"2021-08-02T07:25:49.839131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding\n# features = embed3\n\n# 预测值\n# text stats features\n# test_df = add_textstat_features(test_df)\n# test_df = difficult_words_ratio(test_df)\n# test_df = syllable_ratio(test_df)\n\n# print(test_df.head())\n# test_df.to_csv(\"./mldata.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:25:49.848223Z","iopub.execute_input":"2021-08-02T07:25:49.848498Z","iopub.status.idle":"2021-08-02T07:25:49.855909Z","shell.execute_reply.started":"2021-08-02T07:25:49.848471Z","shell.execute_reply":"2021-08-02T07:25:49.855157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# # !pip install kaggle\n\n# os.environ[\"KAGGLE_USERNAME\"] = \"racleray\"\n# os.environ[\"KAGGLE_KEY\"] = \"d1c80c062506c912e369893b207eaca9\"\n\n# !kaggle datasets metadata racleray/comlitmldata\n# # !mv dataset-metadata.json model/\n\n# # 最好不要有文件夹\n# !kaggle datasets version -p ./ -m \"Updated data base fine\"","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:25:49.857779Z","iopub.execute_input":"2021-08-02T07:25:49.858368Z","iopub.status.idle":"2021-08-02T07:25:49.868652Z","shell.execute_reply.started":"2021-08-02T07:25:49.858331Z","shell.execute_reply":"2021-08-02T07:25:49.8678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ML","metadata":{}},{"cell_type":"code","source":"embed1 = load_embeddings(\"./embed1.bin\")\nprint(len(embed1))\n\nembed2 = load_embeddings(\"./embed2.bin\")\nprint(len(embed2))\n\nembed3 = load_embeddings(\"./embed3.bin\")\nprint(len(embed3))\n\nembed4 = load_embeddings(\"./embed4.bin\")\nprint(len(embed4))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:26:04.79776Z","iopub.execute_input":"2021-08-02T07:26:04.79808Z","iopub.status.idle":"2021-08-02T07:26:04.910218Z","shell.execute_reply.started":"2021-08-02T07:26:04.798052Z","shell.execute_reply":"2021-08-02T07:26:04.909234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVR","metadata":{}},{"cell_type":"code","source":"# rmse_score SVR\nfrom joblib import dump, load\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\n\ntrain_data = test_df\n\n## Reset bins\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:, 'bins'] = pd.cut(train_data['target'], bins=num_bins, labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:26:31.229596Z","iopub.execute_input":"2021-08-02T07:26:31.229943Z","iopub.status.idle":"2021-08-02T07:26:31.262497Z","shell.execute_reply.started":"2021-08-02T07:26:31.229913Z","shell.execute_reply":"2021-08-02T07:26:31.261575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:26:44.156364Z","iopub.execute_input":"2021-08-02T07:26:44.156737Z","iopub.status.idle":"2021-08-02T07:26:44.192233Z","shell.execute_reply.started":"2021-08-02T07:26:44.156704Z","shell.execute_reply":"2021-08-02T07:26:44.191249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\n# input whole train_data not shuffled\ndef embedding_svr_train(df, bins, models_embedding, save_dir=\"./\", emb=1, bert_nums=5, svr_nfolds=10, C=8, kernel='rbf'):\n    mean_scores = []\n    records = []\n\n    # get embeddings\n    #print(models_embedding[0].shape)\n    #print(type(models_embedding[0]))    \n    print(\"Embedding got.\")    \n\n    \n    # SVM training: 5 SVR model\n    for index, X in enumerate(models_embedding):\n        print(f'{by_}{r_}  SVR#{index+1} training {sr_}')\n        scores = []\n        model = SVR(C=C, kernel=kernel, gamma='auto')\n        # new kfold\n        kfold = StratifiedKFold(n_splits=svr_nfolds, shuffle=True, random_state=42)\n        for i, (train_idx, valid_idx) in tqdm(enumerate(kfold.split(X, bins))):\n#             model = SVR(C=C, kernel=kernel, gamma='auto')\n            X_train, y_train = X[train_idx], target[train_idx]\n            X_valid, y_valid = X[valid_idx], target[valid_idx]\n            \n            model.fit(X_train, y_train)\n            \n            prediction = model.predict(X_valid)\n            score = rmse_score(prediction, y_valid)\n            scores.append(score)\n            print(f'\\t\\t{y_}SVR {index} Fold {i} , rmse score: {score:.4f} {sr_}')\n\n        os.makedirs(save_dir, exist_ok=True)\n        dump(model, save_dir + f'svr_{emb}_{index}.bin')\n\n        mean_score = np.mean(scores)\n        print(f'\\t{r_}SVR {index} mean rmse score: {mean_score:.4f} {sr_}')\n        mean_scores.append(mean_score)\n        records.append(scores)\n\n    print(f'Avg rmse score of 5 SVR: {np.mean(mean_scores):.4f}')\n\n    return records","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:27:13.336278Z","iopub.execute_input":"2021-08-02T07:27:13.336674Z","iopub.status.idle":"2021-08-02T07:27:13.347955Z","shell.execute_reply.started":"2021-08-02T07:27:13.33664Z","shell.execute_reply":"2021-08-02T07:27:13.346812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed1, emb=1, svr_nfolds=5, C=100) ","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:27:14.853609Z","iopub.execute_input":"2021-08-02T07:27:14.853973Z","iopub.status.idle":"2021-08-02T07:29:42.928162Z","shell.execute_reply.started":"2021-08-02T07:27:14.853941Z","shell.execute_reply":"2021-08-02T07:29:42.927208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed1, svr_nfolds=5, C=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:29:42.929985Z","iopub.execute_input":"2021-08-02T07:29:42.930332Z","iopub.status.idle":"2021-08-02T07:29:42.93499Z","shell.execute_reply.started":"2021-08-02T07:29:42.930296Z","shell.execute_reply":"2021-08-02T07:29:42.934021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed1, emb=1, svr_nfolds=5, C=200)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:29:42.937402Z","iopub.execute_input":"2021-08-02T07:29:42.93784Z","iopub.status.idle":"2021-08-02T07:29:42.944498Z","shell.execute_reply.started":"2021-08-02T07:29:42.937801Z","shell.execute_reply":"2021-08-02T07:29:42.943474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed2, emb=2, svr_nfolds=5, C=200)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:29:42.946063Z","iopub.execute_input":"2021-08-02T07:29:42.946405Z","iopub.status.idle":"2021-08-02T07:34:09.937115Z","shell.execute_reply.started":"2021-08-02T07:29:42.946369Z","shell.execute_reply":"2021-08-02T07:34:09.936112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed3, emb=3, svr_nfolds=5, C=200)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:34:09.938351Z","iopub.execute_input":"2021-08-02T07:34:09.938853Z","iopub.status.idle":"2021-08-02T07:38:15.305008Z","shell.execute_reply.started":"2021-08-02T07:34:09.938815Z","shell.execute_reply":"2021-08-02T07:38:15.303649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed3, svr_nfolds=5, C=250)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:38:15.307041Z","iopub.execute_input":"2021-08-02T07:38:15.307517Z","iopub.status.idle":"2021-08-02T07:38:15.313717Z","shell.execute_reply.started":"2021-08-02T07:38:15.307447Z","shell.execute_reply":"2021-08-02T07:38:15.312419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_svr_train(train_data, bins, embed4, emb=4, svr_nfolds=5, C=200)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T07:38:15.315943Z","iopub.execute_input":"2021-08-02T07:38:15.316457Z","iopub.status.idle":"2021-08-02T07:42:50.069346Z","shell.execute_reply.started":"2021-08-02T07:38:15.316388Z","shell.execute_reply":"2021-08-02T07:42:50.068461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# # !pip install kaggle\n\n# os.environ[\"KAGGLE_USERNAME\"] = \"\"\n# os.environ[\"KAGGLE_KEY\"] = \"\"\n\n# !kaggle datasets metadata racleray/svrmodel\n# # !mv dataset-metadata.json model/\n\n# # 最好不要有文件夹\n# !kaggle datasets version -p ./ -m \"Updated data base fine\"","metadata":{"execution":{"iopub.status.busy":"2021-08-02T08:22:13.279316Z","iopub.execute_input":"2021-08-02T08:22:13.279696Z","iopub.status.idle":"2021-08-02T08:22:13.284829Z","shell.execute_reply.started":"2021-08-02T08:22:13.27958Z","shell.execute_reply":"2021-08-02T08:22:13.283248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM","metadata":{}},{"cell_type":"markdown","source":"过拟合","metadata":{}},{"cell_type":"code","source":"# ml_df = pd.read_csv(\"../input/comlitmldata/mldata.csv\")\n# ml_df.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Smaller maxbin: reduces train accuracy but has potential to increase generalization\n# Bigger min_data_in_leaf: has potential to reduce overfitting\n\n# params = {\n# 'boosting_type': 'gbdt',\n# 'objective': 'regression',\n# 'metric': 'rmse',\n#     # baseline at 100 for min_data_in_leaf\n# 'min_data_in_leaf': 100,\n#     # baseline at .8\n# 'feature_fraction': .8,\n#     # baseline at .8\n# 'bagging_fraction': 0.8,\n# 'bagging_freq': 10,\n# 'max_depth': 10,\n# 'num_leaves': 32,\n# 'learning_rate': 0.05,\n#     # baseline at max_bin 256\n# \"max_bin\": 100,\n# \"n_estimators\": 10000,\n# }\n\n\n# params = {\n#     'boosting_type': 'gbdt',\n#     'metric': 'rmse',\n#     'objective': 'regression',\n#     'verbose': -1,\n#     'learning_rate': 0.05,\n#     'max_depth': 10,\n#     'feature_pre_filter': False,\n#     'lambda_l1': 2.215942517163985,\n#     'lambda_l2': 0.0015606472088872934,\n#     'num_leaves': 2,\n#     'feature_fraction': 0.8999999999999999,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 30,\n# }\n\n\n# lgm_data = ml_df.copy()\n\n# cols2remove = ['url_legal'\n#                , 'excerpt'\n#                , 'id'\n#                , 'license'\n#                , 'kfold'\n#                , 'bins'\n#                , 'standard_error'\n# #                , 'm2', 'm3', 'm4', 'm1'\n#               ]\n\n# lgm_data = lgm_data.drop(columns=cols2remove)\n\n\n# lgm_data.head()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import lightgbm as lgm\n\n# ## Reset bins\n# num_bins = int(np.floor(1 + np.log2(len(lgm_data))))\n# lgm_data.loc[:, 'bins'] = pd.cut(lgm_data['target'], bins=num_bins, labels=False)\n\n# X = lgm_data.loc[:, lgm_data.columns != 'target'].to_numpy()\n# target = lgm_data['target'].to_numpy()\n# bins = lgm_data.bins.to_numpy()\n\n\n# pred = np.zeros(ml_df.shape[0])\n# rmses = []\n# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# for i, (train_idx, valid_idx) in tqdm(enumerate(kfold.split(X, bins))):\n#     X_train, y_train = X[train_idx], target[train_idx]\n#     X_valid, y_valid = X[valid_idx], target[valid_idx]\n    \n#     lgm_train_set = lgm.Dataset(data=X_train, label=y_train)\n#     lgm_valid_set = lgm.Dataset(data=X_valid, label=y_valid, reference=lgm_train_set)\n    \n#     model = lgb.train(\n#         params,\n#         lgm_train_set, \n#         num_boost_round=1000,\n#         early_stopping_rounds=10,\n#         valid_sets=[lgm_train_set, lgm_valid_set], \n#         verbose_eval=-1\n#     )\n\n#     y_pred = model.predict(X_valid)\n#     rmse = rmse_score(y_pred, y_valid)\n#     rmses.append(rmse)\n    \n#     # tmp_pred = model.predict(X_test)\n#     # pred += tmp_pred / 5\n    \n# print(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Only SVR inference[Test using]","metadata":{}},{"cell_type":"markdown","source":"-   id    target\n- 0  c0f722661 -0.367501\n- 1  f0953f0a5 -0.398820\n- 2  0df072751 -0.537474\n- 3  04caf4e0c -2.299636\n- 4  0e63f8bea -1.962926\n- 5  12537fe78 -1.093182\n- 6  965e592c0  0.077126","metadata":{}},{"cell_type":"markdown","source":"问题在于 需要合适的 embedding， 容易过拟合，需要bagging","metadata":{}},{"cell_type":"code","source":"# # rmse_score SVR\n# from joblib import dump, load\n# from sklearn.svm import SVR\n# from sklearn.metrics import mean_squared_error\n# from sklearn.model_selection import StratifiedKFold\n\n\n# def rmse_score(y_true,y_pred):\n#     return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\n# def embedding_svr_test(df, save_dir, bert_path, bert_nums=5, svr_nfolds=10):\n#     # get embeddings\n#     models_embedding = []\n#     for fold_num in range(bert_nums):\n#         print(f'{by_}{r_}  Model#{fold_num+1} inferencing {sr_}')\n#         device = Config.device\n\n#         test_dataloader = get_test_data(df)\n\n#         model = MeanModelEmbedding()\n#         model.load_state_dict(torch.load(bert_path + f'model_{fold_num}.bin'))\n#         model.to(device)\n#         model.eval()\n\n#         embeddings = []\n#         with torch.no_grad():\n#             for i, batch in tqdm(enumerate(test_dataloader)):\n#                 sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n#                 outputs = model(sent_id, mask)\n#                 outputs = outputs.detach().cpu().numpy()\n#                 embeddings.extend(outputs)\n#             embeddings = np.array(embeddings)\n#         models_embedding.append(embeddings)\n\n#         del model\n#         gc.collect()\n#         torch.cuda.empty_cache()\n        \n#     print(f'Embedding got.')\n\n#     # SVM predict: 5 SVR model\n#     results = np.zeros((df.shape[0]))\n#     for index, X_test in enumerate(models_embedding):\n#         print(f'{by_}{r_}  SVR#{index+1} predicting {sr_}')\n#         for i in range(svr_nfolds):\n#             svr = load(save_dir + f'svr_{index}_{i}.bin')\n#             preds = svr.predict(X_test)\n#             results += preds\n            \n#     print(f'Complete.')\n\n#     return results / bert_nums / svr_nfolds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}