{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tf 2.0 +\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Bidirectional,\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    GlobalAveragePooling1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    Input,\n",
    "    SpatialDropout1D,\n",
    "    concatenate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "train = pd.read_csv(\"../data/train_set.csv\")\n",
    "test = pd.read_csv(\"../data/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000,\n",
    "                                                  lower=False,\n",
    "                                                  filters=\"\")\n",
    "tokenizer.fit_on_texts(\n",
    "    list(train[\"word_seg\"].values) + list(test[\"word_seg\"].values))\n",
    "\n",
    "train_ = tokenizer.texts_to_sequences(train[\"word_seg\"].values)\n",
    "test_ = tokenizer.texts_to_sequences(test[\"word_seg\"].values)\n",
    "\n",
    "train_ = tf.keras.preprocessing.sequence.pad_sequences(train_, maxlen=1800)\n",
    "test_ = tf.keras.preprocessing.sequence.pad_sequences(test_, maxlen=1800)\n",
    "\n",
    "word_vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "lb = LabelEncoder()\n",
    "train_label = lb.fit_transform(train[\"class\"].values)\n",
    "train_label = to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "可加入fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train[\"word_seg\"], test[\"word_seg\"]])\n",
    "file_name = \"../embedding/Word2Vec_word_200.model\"\n",
    "if not os.path.exists(file_name):\n",
    "    model = Word2Vec(\n",
    "        [[word for word in document.split(\" \")] for document in all_data.values],\n",
    "        size=200,\n",
    "        window=5,\n",
    "        iter=10,\n",
    "        workers=11,\n",
    "        seed=2018,\n",
    "        min_count=2,\n",
    "    )\n",
    "    model.save(file_name)\n",
    "else:\n",
    "    model = Word2Vec.load(file_name)\n",
    "print(\"Word2vec loaded....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Glove文件夹下 demo.sh 训练，修改shell文件中参数配置，得到glove_vec.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(\"../embedding/glove_vec.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        glove[word] = vec\n",
    "print(\"Glove loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_w2v_matrix = np.zeros((len(word_vocab) + 1, 200))\n",
    "for word, i in word_vocab.items():\n",
    "    embedding_vector = model.wv[word] if word in model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_w2v_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk_vec = np.random.random(200) * 0.5\n",
    "        unk_vec = unk_vec - unk_vec.mean()\n",
    "        embedding_w2v_matrix[i] = unk_vec\n",
    "\n",
    "\n",
    "embedding_glove_matrix = np.zeros((len(word_vocab) + 1, 200))\n",
    "for word, i in word_vocab.items():\n",
    "    embedding_vector = model.wv[word] if word in model else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_glove_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        unk_vec = np.random.random(200) * 0.5\n",
    "        unk_vec = unk_vec - unk_vec.mean()\n",
    "        embedding_glove_matrix[i] = unk_vec\n",
    "\n",
    "embedding_matrix = np.concatenate((embedding_w2v_matrix, embedding_glove_matrix), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf 2.0 +\n",
    "\n",
    "def build_model(sent_length, embeddings_weight):\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "        name=\"word_embedding\",\n",
    "        input_dim=embeddings_weight.shape[0],\n",
    "        weights=[embeddings_weight],\n",
    "        output_dim=embeddings_weight.shape[1],\n",
    "        trainable=False)\n",
    "\n",
    "    x = SpatialDropout1D(0.2)(embedding(content))\n",
    "\n",
    "    x = Bidirectional(GRU(200, return_sequences=True))(x)\n",
    "    x = Bidirectional(GRU(200, return_sequences=True))(x)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "\n",
    "    x = Dense(1000)(conc)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(500)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    output = Dense(19, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=content, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=666)\n",
    "\n",
    "\n",
    "train_pre_matrix = np.zeros((train.shape[0],19)) #记录验证集的概率\n",
    "test_pre_matrix = np.zeros((10,test.shape[0],19)) #将10轮的测试概率分别保存起来\n",
    "cv_scores=[]\n",
    "\n",
    "for i, (train_fold, test_fold) in enumerate(kf.split(train_)):\n",
    "    print(\"第%s的结果\"%i)\n",
    "    X_train, X_valid = train_[train_fold, :], train_[test_fold, :]\n",
    "    y_train, y_valid = train_label[train_fold], train_label[test_fold]\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(64)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(128)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_,np.zeros((test_.shape[0],19)))).batch(128)\n",
    "\n",
    "    checkpoint_dir = './cv_checkpoints/cv_'+str(i)+'/'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "    model = build_model(1800, embedding_matrix)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=6)\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_accuracy\", verbose=1, mode='max', factor=0.5, patience=3)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_prefix, monitor='val_accuracy', \n",
    "                                 verbose=2, save_best_only=True, mode='max',save_weights_only=True)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        model.fit(train_ds,\n",
    "                  epochs=30,\n",
    "                  validation_data=val_ds,\n",
    "                  callbacks=[early_stopping, plateau, checkpoint],\n",
    "                  verbose=2)\n",
    "\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    valid_prob = model.predict(val_ds)\n",
    "    valid_pred = np.argmax(valid_prob,axis=1)\n",
    "    valid_pred = lb.inverse_transform(valid_pred)\n",
    "\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "    y_valid = lb.inverse_transform(y_valid)\n",
    "\n",
    "    f1_score_ = f1_score(y_valid,valid_pred,average='macro') \n",
    "    print (\"valid's f1-score: %s\" %f1_score_)\n",
    "\n",
    "    train_pre_matrix[test_fold, :] =  valid_prob\n",
    "    test_pre_matrix[i, :,:]= model.predict(test_ds)\n",
    "\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cv_test_result.npy\",test_pre_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "Routings = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = tf.keras.backend.sum(tf.keras.backend.square(x), axis, keepdims=True)\n",
    "    scale = tf.keras.backend.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = tf.keras.backend.conv1d(u_vecs, kernel=self.W)\n",
    "        else:\n",
    "            u_hat_vecs = tf.keras.backend.local_conv1d(u_vecs, kernel=self.W, kernel_size=[1], strides=[1])\n",
    "\n",
    "        batch_size = tf.shape(u_vecs)[0]\n",
    "        input_num_capsule = tf.shape(u_vecs)[1]\n",
    "        u_hat_vecs = tf.reshape(u_hat_vecs, [batch_size, input_num_capsule,self.num_capsule, self.dim_capsule])\n",
    "        u_hat_vecs = tf.transpose(u_hat_vecs,perm=[0, 2, 1, 3]) # [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "        b = tf.zeros_like(u_hat_vecs[:, :, :, 0])               # [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = tf.transpose(b, perm=[0, 2, 1])                 # [None, input_num_capsule, num_capsule] \n",
    "            c = tf.nn.softmax(b)                                # [None, input_num_capsule, num_capsule] \n",
    "            c = tf.transpose(c, perm=[0, 2, 1])                 # [None, num_capsule, input_num_capsule] \n",
    "            s_j = tf.reduce_sum(tf.multiply(tf.expand_dims(c,axis=3) , u_hat_vecs) , axis=2)        \n",
    "            outputs = self.activation(s_j)                      # [None,num_capsule,dim_capsule]\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.reduce_sum(tf.multiply(tf.expand_dims(outputs,axis=2) , u_hat_vecs) , axis=3)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gru_Capsule_Model(sent_length, embeddings_weight,class_num):\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "        name=\"word_embedding\",\n",
    "        input_dim=embeddings_weight.shape[0],\n",
    "        weights=[embeddings_weight],\n",
    "        output_dim=embeddings_weight.shape[1],\n",
    "        trainable=False)\n",
    "    \n",
    "    embed = SpatialDropout1D(0.2)(embedding(content))\n",
    "    \n",
    "    x = Bidirectional(GRU(200, return_sequences=True))(embed)\n",
    "    \n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings, share_weights=True)(x)\n",
    "    capsule = Flatten()(capsule)\n",
    "    \n",
    "    x = Dense(1000)(capsule)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(500)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=\"relu\")(x)\n",
    "    output = Dense(class_num, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=content, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=666)\n",
    "\n",
    "train_pre_matrix = np.zeros((train.shape[0],19)) #记录验证集的概率\n",
    "test_pre_matrix = np.zeros((10,test.shape[0],19)) #将10轮的测试概率分别保存起来\n",
    "cv_scores=[]\n",
    "\n",
    "for i, (train_fold, test_fold) in enumerate(kf.split(train_)):\n",
    "    print(\"第%s的结果\"%i)\n",
    "    X_train, X_valid = train_[train_fold, :], train_[test_fold, :]\n",
    "    y_train, y_valid = train_label[train_fold], train_label[test_fold]\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(64)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(128)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_,np.zeros((test_.shape[0],19)))).batch(128)\n",
    "\n",
    "    checkpoint_dir = './rnncapsule_cv_checkpoints/cv_'+str(i)+'/'\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "    model = Gru_Capsule_Model(1800, embedding_matrix, 19)\n",
    "    val_ds\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=6)\n",
    "    plateau = ReduceLROnPlateau(monitor=\"val_accuracy\", verbose=1, mode='max', factor=0.5, patience=3)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_prefix, monitor='val_accuracy', \n",
    "                                 verbose=2, save_best_only=True, mode='max',save_weights_only=True)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        model.fit(train_ds,\n",
    "                  epochs=30,\n",
    "                  validation_data=val_ds,\n",
    "                  callbacks=[early_stopping, plateau, checkpoint],\n",
    "                  verbose=2)\n",
    "\n",
    "\n",
    "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    valid_prob = model.predict(val_ds)\n",
    "    valid_pred = np.argmax(valid_prob,axis=1)\n",
    "    valid_pred = lb.inverse_transform(valid_pred)\n",
    "\n",
    "    y_valid = np.argmax(y_valid, axis=1)\n",
    "    y_valid = lb.inverse_transform(y_valid)\n",
    "\n",
    "    f1_score_ = f1_score(y_valid,valid_pred,average='macro') \n",
    "    print (\"valid's f1-score: %s\" %f1_score_)\n",
    "\n",
    "    train_pre_matrix[test_fold, :] =  valid_prob\n",
    "    test_pre_matrix[i, :,:]= model.predict(test_ds)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()    \n",
    "    \n",
    "np.save(\"cv_test_result.npy\",test_pre_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.load(\"cv_test_result.npy\")\n",
    "# res_mean = res.mean(axis=0)\n",
    "\n",
    "# test_pred = lb.inverse_transform(np.argmax(res_mean,axis=1))\n",
    "# test['class'] = test_pred\n",
    "# test[[\"id\",\"class\"]].to_csv(\"submission_baseline_capsule_cv.csv\",index=False,header=True,encoding='utf-8')\n",
    "\n",
    "# or 使用stacking，和机器学习例子中的操作是一致的。只是直接使用深度的预测结果进行stacking model的训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit nlp",
   "language": "python",
   "name": "python36864bit023718609e434315a7782a7404fb6072"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
